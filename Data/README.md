# Dataset Structure

This README describes the expected structure of any dataset such that it can be used by the pipeline.

## Calibration Data Per Dataset

Bonn-Dataset:

+ The color images are stored as 640×480 8-bit RGB images in PNG format.
+ The depth maps are stored as 640×480 16-bit monochrome images in PNG format.
+ The color and depth images are already pre-registered using the OpenNI driver from PrimeSense, i.e., the pixels in the color and depth images correspond already 1:1.
+ The depth images are scaled by a factor of 5000, i.e., a pixel value of 5000 in the depth image corresponds to a distance of 1 meter from the camera, 10000 to 2 meter distance, etc. A pixel value of 0 means missing value/no data.

+ Sources:
 + http://www.ais.uni-bonn.de/download/rigidmultibody/
 + http://vision.in.tum.de/data/datasets/rgbd-dataset/file_formats
+ Intrinsic Camera Calibration of the Kinect
```
fx = 525.0  # focal length x
fy = 525.0  # focal length y
cx = 319.5  # optical center x
cy = 239.5  # optical center y

factor = 5000 # for the 16-bit PNG files
# OR: factor = 1 # for the 32-bit float images in the ROS bag files

for v in range(depth_image.height):
  for u in range(depth_image.width):
    Z = depth_image[v,u] / factor;
    X = (u - cx) * Z / fx;
    Y = (v - cy) * Z / fy;
```


| Dataset              | Depth Div. Scale to Meters | f_depth            | p_depth     | f_rgb  | p_rgb
| -------------------- | -------------------------- | ------------------ | ----------- | ------ | -----  
| Bonn chair           |  5000                      | (525.0, 525.0) | (319.5,  239.5) |  -     | -
 
## Quick Start

+ Complete valid dataset structure (see below)
+ Optionally, re-enumerate the dataset files by using [this](https://github.com/simplay/file_renamer/blob/master/renamer.rb) file renamer.
+ Run `.Source/compute_flows/run.sh` to generate the optical flow fields. Those will be stored in the dataset's root directory.
+ Run `.Source/initialize_data/init_data.m` to generated all required pipeline inputs. 

```
Data/
  my_dataset/
  
    # Required Minimum
    # Given well-enumerated image sequence
    1.png
    2.png
    3.png
    ...
    
    # Normalized-well enumered images
    # Generated by Source/compute_flows/
    1.ppm # 1st normalized image
    2.ppm
    3.ppm
    ...
    
    # flow fields generated by using the LDOF method
    # Generated by Source/compute_flows/
    ldof/
      # Backward flow fields
      # Generated by Source/compute_flows/
      bwf_2LDOF.flo # flow 2 -> 1
      bwf_3LDOF.flo # flow 3 -> 2
      ...
    
      # Forward flows
      # Generated by Source/compute_flows/
      fwf_1LDOF.flo # flow 1 -> 2
      fwf_2LDOF.flo # flow 2 -> 3
      ...
    
    # flow files generated by another supported flow method
    other_flow_file_dir/
      flow files
      ...
    
 
    
    # Optional (well-enumerated) depth maps input
    depths/
      1.png
      2.png
      ...
    
    # Optional meta information
    # Used when using depth cues
    meta/
      
      # extrinsic camera parameters: align depth-and color camera
      calib.txt
      
      # image resolution
      dataset.txt
  
    # Sorted filename list of input data used by pipeline
    # Generated by Source/compute_flows/
    used_input.txt  
    
```

## Detailed explanation

Every available dataset has to be contained in its own subfolder within `./Data/`. 

A **minimal** valid **dataset** consists of a coherently **well-enumerated** (color) image sequence (i.e the frames of a video).
In our case, Well-enumerated means that **n-th frame** of the image sequence has the **name** **n**.
Note that the pipeline offers a renaming script which renames the images files that they are _well-enumerated_.
This script can be found at `.Source/normalize_sensor_data/`.

Currently, **.png** and **.ppm** images are supported.

**Example**: Assume we have a dataset called _foo_, consiting of 3 png images, called `1.png, 2.png, 3.png`. 
Then we have to:

1. Create a subfolder `./Data/foo/`.
2. Put the png images into `foo/`.
 
After having defined a valid dataset for which we want to extract its motion segmentation, we initially have to generate its optical flow fields by running `.Source/compute_flows/run.sh`. This script guides the user through the generation process. Please also read the corresponding readme located at `.Source/compute_flows/`.

The followind additional cues can be fed into the pipeline, which enables additional, accurater motion segmentation extraction modes within the pipeline:

+ **Depth fields**: One depth field per color image. Each such depth field is supposed to be well-enumerated and should be put into `./Data/my_dataset/depth`. Have a look at [here](http://www.ais.uni-bonn.de/download/objecttracking.html) or [there](http://vision.in.tum.de/data/datasets/rgbd-dataset/file_formats) to obtain more information about the used depth maps.
+ **Extrinsic Camera Clibration Parameters**: Put a file called `Dataset/my_dataset/meta/calib.txt`. The content of such a file looks like the following:

```
f_d: <depth intrinsic params: focal length x y>
p_d: <depth intrinsic params: principal point x y>
f_rgb: <rgb inntrinsic params: focal length x y>
p_rgb: <rgb inntrinsic params: principal point x y>

# The Extrinsic parameters 4x3 matrix describing
the transformation from color to depth

e_1: <1st row of the extrinsic 4x3 matrix>
e_2: <2nd row of the extrinsic 4x3 matrix>
e_3: <3rd row of the extrinsic 4x3 matrix>

```

Note that the order of the labels does not matter, the parser knows how to handle various line order.
It only matters, that the correct separators are used.

The following regex expressed the expected format: `<LABEL_ID>:(\s<NUM>){2,3}`
So, white-spaces (`\s`) do matter and also the `:`.

**Example calib.txt** file: 

```
f_d: 504.261 503.905
p_d: 352.457 272.202
f_rgb: 573.71 574.394
p_rgb: 346.471 249.031
e_1: 0.999749 0.00518867 0.0217975 0.0243073
e_2: -0.0051649 0.999986 -0.0011465 -0.000166518
e_3: -0.0218031 0.00103363 0.999762 0.0151706

```

By Running the a valid dataset on `./Source/init_data/init_data.m` we will generate all the required pipeline inputs. Please read the corresponding docs of the script `init_data.m`. 

## Additional Info

to tranform depth data to srsf compatible depth files, run a matlab script similar to this one:

```matlab

for k=1:101, 
    d = imread(strcat('depth/',num2str(k),'.png')); 
    d = uint16(double(d)*0.2); 
    imwrite(d,strcat('depth2/',num2str(k),'.png')); 
end

```

The srsf compatible depth files are then stored in the directory `depth2/`. Keep in mind to swap the directory namees of `depth` and `depth2` before running the SRSF flow computation code and afterwards changing it back.
TODO: Do this automatically.
