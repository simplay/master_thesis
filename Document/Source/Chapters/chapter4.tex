\chapter{Implementation}
In this chapter we explain in detail the relevant stages of our motion segmentation pipeline. We start by formulate the initial problem statement our pipeline tries to solve. Moreover, for each stage, we describe its required input data and what output it generates. Lastly, we also mention all assumptions and induced limitations for each pipeline stage. \\ \\
The problem statement our pipeline solves is defined as follows: 
Given a set of images that form a video sequence and their associated depth maps. Our goal is to separate the images into regions that form coherent and independent rigidly moving objects. \\ \\
%SHOW FIGURE OF PROBLEM STATEMENT
% INPUT: IMGS, DEPTHS, OUTPUT: SEGMENTATION
% SHOW LATER A DETAILED PIPELINE
The idea of such a segmentation is to identify and extract the meaningful rigid motions from the background. To accomplish this task we compute the optical flow on the images and use it as a guide for grouping pixel regions that belong together. Since the object motion in coherent image sequence is not independent for every frame, the optical flow depicts an ideal cue for making grouping decisions. \\ \\
Our pipeline has the following main stages:

\begin{enumerate}
\item \textbf{Dataset Preparations}: Initially, the frames of a video have to be extracted and named according to the pipeline naming conventions. If present, also the depth maps have to be named and re-normalized according to the pipeline's conventions. Optionally, a blurred version of the input images are computed by using a bilateral filter. This filtered images act as an additional, but optional cue, for a later pipeline stage. 
\item \textbf{Optical Flows}: We compute the forward- and backward-flows on our input sequence using different existing implementations. Our pipeline lets the user select a target method for generating the optical flow fields.
\item \textbf{Data Extraction}: In this stage we extract traceable feature locations in our images. Also, a mask containing all invalid tracking locations per image is computed by checking whether the the forward and backward flow correspond to each other. Optionally, depth data, flow and depth variances and color maps are extracted that are used within a later pipeline stage. 
\item \textbf{Trajectories}: We use the computed forward flows and the previously extracted traceable feature locations to perform a point tracking. A sequence of traced points form a so called trajectory. In every frame a trajectory can either be started, ended or be continued. The previously computed traceable locations act as the starting points, the forward flow is added to a traceable location and yield the tracked to position, the starting location in the next frame. Tracked to locations that land in an invalid mask location cause the tracking of a trajectory. 
\item \textbf{Affinity Matrix}: In this stage we compute the similarities between all trajectory pairs. The similarity is computed according to different metrics, which are basically a combination of various distances among overlapping trajectory parts. 
\item \textbf{Sparse Motion Segmentation}: Using the affinity matrix plus the nearest neighbouring trajectories per trajectory, we can compute its dense segmentation by either applying a spectral clustering on the affinity matrix or by reformulating the problem as a graph cut problem. Usually 
\item \textbf{Dense Segmentation}: Our pipeline allows to transform the sparse segmentation into a dense segmentation. For this purpose we consider the hole filling problem, formulating it as a convex problem and then solving it using a primal-dual approach using the sparse segmentation as the initial input.
\item \textbf{Evaluation}: Our pipeline allows to qualitatively and quantitatively evaluate the generated sparse and dense segmentations. In addition, we also can explore various parameter settings and their outcomes using different visualizers.
\end{enumerate}
In the following sections we examine and discuss each individual pipeline stage in detail. 

\section{Dataset Preparations}
The preparation of an input datasets is the very first stage of our pipeline. Initially, we have to either capture a video using one of our capturing devices or use an existing video sequence. Next, we extract all the frames from the considered video. \\ \\
In case there are also depth maps$\footnote{In case we are working with depth data, we assume that there exists one depth map per frame. Our pipeline assumes, that the values in the depth images are in meter units}$ available, they are transformed such that the value range is in meter units. For further information about the dataset conventions, read section~\ref{sec:datasets} on page~\pageref{sec:datasets}. \\ \\
Optionally, a filtered version of the input dataset images can be generated using a Bilateral Filter$\footnote{A Bilateral Filter is a non-linear, edge-preserving blur filter.}$. These filtered images can help improve the process of finding traceable candidate location as well as identifying invalid tracking locations.

\section{Computing Optical Flows}

Optical flows are used to trace coherent tacking points that form a trajectory.

Four different methods are available:
Large displacement optical flow (LDOF)
Original flow method proposed by Horn and Schunck (HS)
Semi-rigid scene flow (SRSF)
Layered RGBD flow (LRGB)


\section{Data Extraction}

\paragraph{Initial Points}
a set of points is initialized in the first video frame.
principally, every pixel location could be used. however, homogeneous areas can be problematic for the optical flow. therefore, only reliable points are kept and we remove points that doe not show any structure in their vicinity based on the smaller eigenvalue of the structure tensor. 

for efficiency reasons, we spatially subsample the initial points.
factors larger than 12 lose details as there are not enough points to cover small object parts. on the other hand, factors smaller than 4 waste computation time as smaller objects tend to get smoothed away.




Traceable candidate locations are obtained by applying a Harris Corner Detector on the input image. 
Features that are identified as corners are a viable option for starting a trajectory tracking.

in addition, a boolean grid with a certain cell size is applied to the candidates, acting as a selection mask. This reduces the number of tracking candidates (spacial subsampling) drastically and makes the sampling sparser.


mention how invalid regions are computed: occluded regions.

\paragraph{Occlusion detection}
idea: stop tracking points as soon as a point gets occluded.

in tracking, occlusion is usually detected by comparing the appearance of the local neighborhood of the tracked point over time. in contrast, we detect occlusions by verifying the consistency of the forward and the backward flow.

Occlusion is detected by applying the backward flow to a continued tracked point and checking whether that resulting point has the same origin as the actual tracked from position. See figure FIGURE. In non-occluded case, the backward flow vector points in the inverse direction of the forward flow vector. if this consistency required is not satisfied, the point is either getting occluded at frame $t+1$ (the next frame) or the flow was not correctly estimated. both are good reasons to stop tracking this point at $t$. Since there are always some small estimation errors in the optical flow, there is a small tolerance interval granted.
Occlusion comes with the opposite phenomenon disocclision or scaling. to fill these areas not covered by trajectory yet, new trajectories are initialized, in empty areas in each new frame using the same strategy as for the first frame.

the color values are stored as cie lab color images.



depth fields
color values
flow-and depth field variances.

\section{Trajectories}

every point in frame $t$ can be tracked to the next frame $t+1$ by suing the optical flow field $w_t$ that belongs to frame $t$. In principal, any optical flow method can be used.

tracking data has to be stopped as soon as a point gets occluded. otherwise, the point trajectory will share the motion of two different objects. For this purpose we make use of the invalid mask computed earlier.

Occlusion comes with the opposite phenomenon disocclision or scaling. to fill these areas not covered by trajectory yet, new trajectories are initialized, in empty areas in each new frame using the same strategy as for the first frame.
However, most trajectories are never due to disocclusion.


problems of optical flow methods yield bad segmentations. known problems: large displacements, sharp discontinuities, accuracy for the occlusion detection.

\section{Affinity Matrix}
\section{Sparse Motion Segmentation}
There are three different segmentation methods available
Spectral clustering of the affinity matrix
MinCut
Kernighan-Lin 
\section{Dense Motion Segmentation}
%TODO: use the dense segmentation as an input solve the problem of whole filling using a primal-dual convex algorithm. 


In this section I describe how I have actually Implemented the so far described dual-primal solver for demosaicing a raw image.

One important note in advance. In the discrete case, the following holds true 

\begin{align}
	\nabla^* (y^{n+1}) 
	&= \nabla^{T} (y^{n+1}) \nonumber \\
	&= div(y^{n+1})
\end{align}

So we have to omit a minus one factor. This affects the update rule for $x^{n+1}$ derived previously.

In the previous section we have defined explicit update rules. Aggregating all finding, mainly those from equation $\ref{eq:update_x_n_p_1}$ and $\ref{eq:update_rule_y_n_p_1}$, and plugging them into equation $\ref{eq:update_rules_plain}$ we get our update rules


\begin{align}
	y^{n+1} &= \frac{y^n + \sigma \nabla \bar{x}^{n}}{\max{\left(1,\twonorm{y^n + \sigma \nabla \bar{x}^{n}} \right)}} \nonumber \\
	x^{n+1} &= \frac{x^n - \tau div(y^{n+1}) +  \tau \lambda \Omega g}{1+\tau \lambda \Omega} \\
	\bar{x}^{n+1} &= x^{n+1} + \theta(x^{n+1} - x^n)
\label{eq:final_update_rules_plain}	
\end{align}

I initialized $x_n$ with the mosaiced image $g$, $y^{n}$ with a zeros \footnote{a tensor of dimension $M \times N \times 2$ filled with zeros, where $(M \times N)$ denotes the dimension of one color channel of $g$.} and $\bar{x}^{n}$ also with the mosaiced image $g$.

For computing $\nabla$ I used a forward difference approximation scheme. For computing the divergence operator of the vector-field $y^{n+1}$ I used backward difference approximation scheme. The reason for using a backward difference using a backward difference is to shift back gradients (remember, the divergence is applied to $y^{n+1}$ which is the result of a forward difference. Otherwise, when not altering between a forward-and backward difference we would end up with shifted gradients.

For computing the divergence, I relied on its mathematical definition. For a given vector-field $v = (v_x, v_y)$ the divergence is defined as the following:

\begin{align}
	div(v) = \partial_x v_x + \partial_y v_y
\end{align}

Since in our case we have $v = y^{n+1}$ and $y^{n+1}$ a vector valued function of the form $y^{n+1} = (y_{x}^{n+1}, (y_{y}^{n+1})$ it follows:

\begin{align}
	div(y^{n+1}) 
	&= div((y_{x}^{n+1}) + div((y_{y}^{n+1}) \\
	&= \left( \partial_x y_{x}^{n+1} + \partial_y y_{x}^{n+1} \right) + \left( \partial_x y_{y}^{n+1} + \partial_y y_{y}^{n+1} \right)
\end{align}

Initially, I used the following parameter setting:

\begin{align}
	\lambda &= 1000 \\
	\theta &= 0.5 \\
	\tau &= 2*10^{-3} \\
	\sigma &= \frac{1}{\tau * \sqrt{\norm{K}}}
\label{eq:parameter_set_up}	
\end{align}

With $\sqrt{\norm{K}} = \sqrt{4}$, a strong upper bound for the function $K$\footnote{A proof for the existence of this upper bound can be found in \cite{chambolle2004algorithm}}.

For consistency, a named all functions in my Matlab code the same as in this report. Furthermore I used a fixed number of iterations for computing my iterative demosaiced images.

the final algorithms I have to perform is the following:

For each color-channel $C \in \{R,G,B\}$ Do: Loop until $\norm{\bar{x}^{n+1} - \bar{x}^{n}}$ is small enough do: use parameter setup as defined in equation $\ref{eq:parameter_set_up}$ and then solve the update rules from equation $\ref{eq:final_update_rules_plain}$. Finally, merge all color iterative color channel solutions to a color image.

When computing the gradient and divergence finite approximation schemes, I used a zero padding boundary condition. Since I also tried out this kind of boundary condition in the first report, comparing the results produced in this project which those from the first project is valid.

One last comment: From the definition of the update rules, we see that the value of $\lambda$ directly affects the parameters $\tau$ and $\sigma$. Thus, when changing the value of $\lambda$ we also would have to find new best $\tau$ and $\sigma$ parameters. Hence, changing $\lambda$ also affects the convergence behaviour of the primal dual method. 

\section{Evaluation Program}
statistically evaluate the quality






mention pipeline assumptions
discuss the details about each pipeline stage
mention in which language the individual parts are coded.
