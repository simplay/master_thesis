%TODO: where should we use this: problems of optical flow methods yield bad segmentations. known problems: large displacements, sharp discontinuities, accuracy for the occlusion detection.
% TODO mention pipeline assumptions
% TODO mention in which language the individual parts are coded.
% TODO check where this might belong to: the color values are stored as CIE lab color images.
\chapter{Implementation}
In this chapter we explain in detail the relevant stages of our motion segmentation pipeline. We start by formulate the initial problem statement our pipeline tries to solve. Moreover, for each stage, we describe its required input data and what output it generates. Lastly, we also mention all assumptions and induced limitations for each pipeline stage.
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.9\linewidth] {implementation/pipeline}
\end{center}
\caption[Motion Segmentation Pipeline]{A schematic illustration of the structure of our pipeline. Individual pipeline components are drawn as orange boxes. The initial input is drawn as green boxes, the resulting final output as a blue box. Intermediate outputs are drawn as gray boxes. The arrows indicate the data-flow, whereas the white dots indicate a data-flow multiplexing. That means, data that flows through such a dot is used by several pipeline components.}
\label{fig:pipeline_schematic}
\end{figure}
The problem statement our pipeline solves is defined as follows: Given a set of images that form a video sequence and their associated depth maps. Then, our goal is to separate the images into regions that form coherent and independent rigidly moving objects. Additionally, the implementation should be robust to noise and missing data, should be able to handle scene captured by a shaking camera and lastly, it should be able to detect several and fast moving rigid object.\\ \\
The idea of such a segmentation is to identify and extract the meaningful rigid motions from the background. To accomplish this task we first compute the optical flows of the images and use them for grouping pixel regions that belong together. Since the object motion in coherent image sequence is not independent for every frame, the optical flow depicts an ideal cue for making grouping decisions. \\ \\
As shown in figure $\ref{fig:pipeline_schematic}$, our pipeline consists of the following main stages:
% TODO: checks this list and rework it
\begin{enumerate}
\item \textbf{Dataset Preparations}: Initially, the frames of a video have to be extracted and named according to the pipeline naming conventions. If present, also the depth maps have to be named and re-normalized according to the pipeline's conventions. Optionally, a blurred version of the input images are computed by using a bilateral filter. This filtered images act as an additional, but optional cue, for a later pipeline stage. 
\item \textbf{Optical Flows}: We compute the forward- and backward-flows on our input sequence using different existing implementations. Our pipeline lets the user select a target method for generating the optical flow fields.
\item \textbf{Data Extraction}: In this stage we extract traceable feature locations in our images. Also, a mask containing all invalid tracking locations per image is computed by checking whether the the forward and backward flow correspond to each other. Optionally, depth data, flow and depth variances and color maps are extracted that are used within a later pipeline stage. 
\item \textbf{Trajectories}: We use the computed forward flows and the previously extracted traceable feature locations to perform a point tracking. A sequence of traced points form a so called trajectory. In every frame a trajectory can either be started, ended or be continued. The previously computed traceable locations act as the starting points, the forward flow is added to a traceable location and yield the tracked to position, the starting location in the next frame. Tracked to locations that land in an invalid mask location cause the tracking of a trajectory. 
\item \textbf{Affinity Matrix}: In this stage we compute the similarities between all trajectory pairs. The similarity is computed according to different metrics, which are basically a combination of various distances among overlapping trajectory parts. 
\item \textbf{Sparse Motion Segmentation}: Using the affinity matrix plus the nearest neighbouring trajectories per trajectory, we can compute its dense segmentation by either applying a spectral clustering on the affinity matrix or by reformulating the problem as a graph cut problem. Usually 
\item \textbf{Dense Segmentation}: Our pipeline allows to transform the sparse segmentation into a dense segmentation. For this purpose we consider the hole filling problem, formulating it as a convex problem and then solving it using a primal-dual approach using the sparse segmentation as the initial input.
\item \textbf{Evaluation}: Our pipeline allows to qualitatively and quantitatively evaluate the generated sparse and dense segmentations. In addition, we also can explore various parameter settings and their outcomes using different visualizers.
\end{enumerate}
In the following sections we examine and discuss each individual pipeline stage in detail. 

\section{Dataset Preparations}
The preparation of an input datasets is the very first stage of our pipeline. Initially, we have to either capture a video using one of our capturing devices or use an existing video sequence. Next, we extract all the frames from the considered video. \\ \\
In case there are also depth maps$\footnote{In case we are working with depth data, we assume that there exists one depth map per frame. Our pipeline assumes, that the values in the depth images are in meter units}$ available, they are transformed such that the value range is in meter units. For further information about the dataset conventions, read section~\ref{sec:datasets} on page~\pageref{sec:datasets}. \\ \\
Optionally, a filtered version of the input dataset images can be generated using a Bilateral Filter$\footnote{A Bilateral Filter is a non-linear, edge-preserving blur filter.}$. These filtered images can help improve the process of finding traceable candidate location as well as identifying invalid tracking locations.

\section{Computing Optical Flows}
\label{sec:impl_optical_flow}
In this section we give a brief introduction to the flow methods that can be utilized in our pipeline. Flow fields are used to perform the point tracking to extract motion trajectories, to detect occlusions and for computing the affinities between trajectories. Therefore, the quality of the flow fields highly affects the quality of the motion segmentation. \\ \\
A conceptual idea as well as a mathematical formulation is provided in section $\ref{sec:optical_flow}$ on page $\pageref{sec:optical_flow}$. \\ \\
Our pipeline strictly relies on existing implementations of the following four flow methods: Large displacement optical flow (\textbf{LDOF}), a tweaked version of the original flow method proposed by Horn and Schunck (\textbf{HS}), Semi-rigid scene flow (\textbf{SRSF}) and Layered RGBD flow (\textbf{LRGB}). \\ \\
An example output generated by these method when running the same frame sequence is shown in figure $\ref{fig:flow_method_flows}$.  
\begin{figure}[H]
\begin{center}
\subfigure[HS]{
   \includegraphics[width=0.47\linewidth] {implementation/flow_methods/hs}
   \label{fig:flow_method_flows_a}
}
\subfigure[LDOF]{
   \includegraphics[width=0.47\linewidth] {implementation/flow_methods/ldof}
   \label{fig:flow_method_flows_b}
}
~
\subfigure[LRGBD]{
   \includegraphics[width=0.47\linewidth] {implementation/flow_methods/lrgbd}
   \label{fig:flow_method_flows_c}
}
\subfigure[SRSF]{
   \includegraphics[width=0.47\linewidth] {implementation/flow_methods/srsf}
   \label{fig:flow_method_flows_d}
}
\end{center}
\caption[Flow Method Flows]{An visualization of the flow fields produced by our used flow methods when running them on the same frame sequence. Please notice that the color encoding is the same as introduce in section $\ref{sec:optical_flow}$ on page $\pageref{sec:optical_flow}$.}
\label{fig:flow_method_flows}
\end{figure}
In the reminder of this section we summarize the used flow methods.

\subsection{HS}
In $\cite{Deq10}$ Dequing Sun et al studied various flow methods and models and determined the most important factors that increase the quality of produced flows. They incorporated their findings in Horn and Schunck's original formulation and contributed a efficient implementation of their method. Additional information about their formulations can be found in section $\ref{sec:hs_formulation}$ on page $\pageref{sec:hs_formulation}$. \\ \\
When using the abbreviation HS we strictly refer to their implementation. \\ \\
The original HS formulation combines a data term that assumes constancy of some image property with a spatial term that models how the flow is expected to vary across the image. The HS model relies on brightness constancy and spatial smoothness assumptions. Unfortunately it is not very robust to outliers. Therefore, the following main improvements were implemented in their methods:
\begin{itemize}
  \item The quadratic penalty function is replaced by the Charbonnier penalty $p\left( x \right) = \sqrt{x^2 + \epsilon^2}$. This penalty is supposed to be more error-prone to outliers.
  \item An incremental multi-resolution technique is applied to estimate flow fields with large displacements. The optical flow estimated at a coarse level is used to warp the second image toward the first at the next finer level and a flow increment is calculated between the first image and the warped second image. Each level is recursively downsampled by applying a Gaussian filter from its nearest lower level.
  \item An enhanced objective function is used, which including a non-local term that robustly integrates flow estimates over large spatial neighborhoods. This is achieved by applying a median filtering to denoise the flow after every warping step to increase the accuracy.
\end{itemize}
In summary, this flow estimation method is a tweaked version of HS original formulation. The authors offer a freely available Matlab implementation that has a decent run-time. Please note that this implementation does not make use of depth maps to compute the flow estimations.

\subsection{LDOF}
Before the work in $\cite{Bro11a}$, there was no accurate method present to estimate fast motions of small objects, i.e. large motions. Most flow methods are based on a combination of HS flow formulation and the concept of coarse-to-fine image warping. However, both approaches have been extended by robust statistics to fight outliers and the same time retain smoothness assumptions. \\ \\
Two fundamental ideas exist to address the issue of large motions. On on hand there are coarse-to-fine techniques. Such techniques, however, require a downsampling. Unfortunately, this also removes details that may be important for the flow accuracy. consequently, the method cannot refine the flow of structures that are smaller than their displacement, because the structure is smoothed away. Flows produced this way are often close to the motion of the larger scale structures. \\ \\
On the other hand descriptor matching techniques are well suited to match large displacements. However, their resulting correspondences are very sparse, have limited accuracy and exhibit many outliers to to missing regularity constraints and lastly, there is only pixel-level accuracy. \\ \\
In their paper they propose a method how to combine both strategies. They direct a variational technique using correspondences from sparse descriptor matching. The correspondences determined by the descriptors are directly integrated into the variational approach. The allows to make use of all image information at every level and smoothly scales down the influence of descriptor keypoints as the grid gets finer. \\ \\
In summary, LDOF implements a flow estimation method that is capable to handle large motions and thus yields a better quality fast moving objects flow estimates. This method, however, does also not make use of depth fields. The authors offer a freely available, compiled C++ program. The program has an intermediate runtime of about 30 seconds per 480 x 640 pixel frame.

\subsection{SRSF}
In their paper $\cite{Bro14}$, Quiroga et al. proposed an approach to estimate the scene flow$\footnote{The term scene flow denotes a motion field in the 3d space, which can be computed from a single view when using RGB-D sensor data.}$ by exploiting the properties of motion in real world scenes. For that purpose they implemented an over-parameterized framework that estimates scene flows from RGBD images. \\ \\
This allows for piecewise smooth solutions using a TV regularization on the parameterization. Most real world scenes can be modelled as locally or piecewise rigid. This means that the scene is composed of 3d independently rigid components. Moreover, their formulation they offers a general formulation to solve for the local and global motion by jointly using intensity and depth data. \\ \\
They model the scene flow as a 3d vector field consisting of a global rigid motion plus a non-rigid residual. this is particularly useful, when estimating the motion of deformable objects in conjunction with a moving camera. A piecewise smooth solution is estimated using a total variation on the parameterization.\\ \\
In the following some details according to their implementation
\begin{itemize}
  \item they represent motions as a field 3d vector field of twists, which encourages piecewise-smooth solutions of rigid body motions.
  \item instead of representing the flow vectors in the 3d space, they represent the scene structure in the image domain. Then, color and depth data can be coupled using a projective function. this way depth influences the motion in the image domain and consistency constraints can be formulated jointly over the color and depth images. 
  \item However, scene flow estimation using intensity and depth is an ill-posed problem and regularization is needed. In their work the use an over-parameterization of the scene flow. each scene point is allowed to follow a rigid body motion. this way, the regularization can be done on a field of rigid motion.
  \item usually, the 3d motion vector if each point is solved to minimize color and intensity constraints in a data term and the whole 3d motion field is regularized to get spatially smooth solutions while preserving discontinuities. since depth data is available a weighted regularization can be used to preserve motion discontinuities along depth edges. 
  \item In order to formulate an efficient 3d motion exploration they use a parameterization provided by RGB-D sensors to formulate an efficient 3d motion exploration. Hence, they define a warping function to couple the twist motion and the optical flow. They use a warping function to locally constrain the rigid motion field in the image domain and define a depth consistency constraint to exploit the sensor data.
\end{itemize}

The authors offer a partially available code basis that can be compiled to a executable program. The program can only generate motion segmentations for dataset that have a image resolution of 480 by 640 pixels. Since the program makes use of depth fields, corresponding depth maps have to be provided in the pipeline's formatting conventions. A detailed descriptions about their pipeline conventions can be found in their readme. 

\subsection{LRGBD}
In $\cite{Deq10}$ Dequing Sun et al present a layered RGBD scene flow estimation  method. They make use of the fact that depth information allows to recover 3d motion from a single view. However, in that case, depth boundaries are not well aligned with RGB images. Especially in occluded regions methods produce large errors. As a remedy, they therefore, use the depth fields for occlusion reasoning and formulate a layered RGBD scene flow method that jointly solves for the scene segmentation and the motion. In their formulation, the depth fields are used to estimate the per-layer 3d rigid motion to constraint the motion of each layer. \\ \\
The implementation of their method is freely available on the authors website, written in Matlab. In the following we use the abbreviation LRGBD for flows produced by their implementation. \\ \\

In summary LRGBD segments motion into layers ordered according to the depth ranking. The authors offer a freely available Matlab implementation which takes about 10 minutes per 640 x 480 pixel frame to compute its forward and backward flow.

%
%s1
%estimate general 2D motion by decomposition into several layers, which enables occlusion reasoning

%s2
%Other methods do segment the scene to impose rigidity or strong regularization over the regions (or segments). However, these segmentations are only used as tools to improve the accuracy of the estimates, and do not really cor-
%respond to the underlying/independent motions of the scene => partitions the scene into depth layers
%Therefore, the segmentation-from-motion problem, which can be particularly useful for scene understanding or human-machine interaction, is not truely addressed by these methods.

\section{Data Extraction}
In this section we offer the reader a detailed description what data- and how it is extracted. This pipeline stage can be understood as a pre-processing stage. \\ \\
In particular we will explain how meaningful traceable locations of moving objects are determined, how we handle occlusions develop a technique to normalize the flow fields to dump erroneous flows. Algorithm $\ref{alg:data_extraction}$ gives an overview of the involved steps in this stage.
\begin{algorithm}[H]
\caption{Data Extraction}
\begin{table}[H]
  \begin{tabular}{@{}lll@{}}
    \textbf{Input:} & Dataset Images \bf{F} \\
		& Forward- and Backward Optical Flows \bf{OF} \\
 		& Depth Images (\emph{optional}) \bf{DI} \\
    \textbf{Output:} & Traceable Feature Locations \bf{TF} \\
    & Occluded Location Masks \bf{OM}\\
    & Cie-Lab Color Images \bf{CI} \\
    & Flow Variances \bf{FV} \\
    & Depth Variances \bf{DV} \\
    
  \end{tabular} 
\end{table}
\setlength{\fboxrule}{0pt} 
\begin{boxedminipage}{1.0\textwidth}
  \begin{algorithmic}[1]
      \ForAll{$\text{frame } f \in \bf{F}$}
        \State $\text{Run Thresholded Harris Corner Detector}.$
		\State $\text{Append Sampled Features to } \bf{TF}.$
		\State $\text{Fetch } fwf,bwf \in \bf{OF} \text{ that belongs to f}.$
		\State $\text{Apply forward-backward-flow check on fwf and bwf} \bf{TF}.$
		\State $\text{Append invalid check locations to } \bf{OM}.$
		\State $\text{Apply a special variant of the bilateral filter on fwf}.$
		\State $\text{Append filtered flow to } \bf{FV}.$
		\State $\text{Transform f to CIE Lab Colorspace and append it to } \bf{CI}.$
		\State $\text{Fetch } df \in \bf{DI} \text{ that belongs to f}.$
		\State $\text{Apply a 2-pass special variant of the bilateral filter on } df$
		\State $\text{Append the filtered depth to } \bf{DV}$
      \EndFor
  \end{algorithmic}
  \end{boxedminipage}
  \vskip1.5pt
\label{alg:data_extraction}
\end{algorithm}
For every dataset frame we extract potential tracking candidate, the forward-and backward flow to the next frame, the occlusion map and colors in the CIE lab space, the normalized depth fields ranging in meter units and the depth-and flow variances. All this data is then saved to an individual file and used in a later pipeline stage.
\subsection{Tracking Candidates}
% TODO: see, where this might fit
%we further reduce the number of samplings by only taking every $n-th$ sample location.
% therefore, only reliable points are kept and we remove points that doe not show any structure in their vicinity based on the smaller eigenvalue of the structure tensor. 
In this section we explain our approach how we determine image locations that exhibit meaningful motion information and thus are good candidates to track. The found tracking candidates are later used as the staring position by our point tracker and thus responsible for detecting moving objects. \\ \\
The simplest idea would be to track every pixel location. On one hand, this would indeed capture every moving object. On the other hand, however, this also would make the motion tracking task highly inefficient, yet infeasible for common image resolutions. Therefore, we want a method which offers us a sparse sampling of the image locations but the same time hits enough meaningful candidates. \\ \\
Ideally, we want to use a feature detector that ignores points in homogeneous areas but selects points that show structural information in their vicinity. Since image corners exhibit a lot of structural information a Harris detector is a solid choice to approach this task. \\ \\
The idea of this detector is the following: A corner defines an intersection of two edges. Therefore, it represents a point in which the directions of these two edges change. For images this means that the gradient has a large variance at that location. \\ \\
It is possible to describe the average intensity change in any direction $(x,y)$
as a bilinear from
\begin{equation}
\left( x,y \right) M \colvec{x}{y}	
\end{equation}
Within the Harris detector code, every image location is then described in terms of the eigenvalues of $M$. Moreover, the detector uses the pixel-wise eigenvectors to measure the corner response. Good corners have a large intensity change in all directions thus their measured response should be large and positive. A detailed formulation can be found in section $\ref{sec:harris_corner_detector}$ on page $\pageref{sec:harris_corner_detector}$. \\ \\
We apply the described Harris corner detector on every dataset image. This procedure yields a sparse set of traceable corner candidates per frame. Additionally, candidates with a too weak corner response are filtered according to a certain threshold value. \\ \\
Lastly, for efficiency reasons, we spatially subsample the extracted locations. The subsampling is performed by applying a boolean grid with a certain cell size to the candidates, acting as a selection mask. This reduces the number of tracking candidates drastically and makes the sampling sparser. \\ \\
The image locations of the remaining candidates are then dumped into a file. An example of this tracking candidate extraction procedure is illustrated in figure $\ref{fig:tracable_candidates}$. \\ \\
\begin{figure}[H]
\begin{center}
\subfigure[Input Image]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/01}
   \label{fig:c14_f1_cand}
}
\subfigure[Sparse Candidates]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/candidates_sparse}
   \label{fig:c14_cand_sparse}
}
~
\subfigure[Harris Corners]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/out_corners}
   \label{fig:c14_cand_corners}
}
\subfigure[Dense Candidates]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/c14_f1_candidates}
   \label{fig:c14_cand_dense}
}
\end{center}
\caption[Tracking Candidates]{A visualization of the tracking candidates extraction stages. For a given input image as shown in subfigure $\ref{fig:c14_f1_cand}$ we want to compute a sparse set of traceable feature locations as shown in subfigure $\ref{fig:c14_cand_sparse}$.}
\label{fig:tracable_candidates}
\end{figure}
For a given dataset frame as shown in subfigure $\ref{fig:c14_f1_cand}$ we compute its corner candidates as illustrated in figure $\ref{fig:c14_cand_corners}$. As mentioned earlier, too weak corner candidates are rejected by filtering them. The set candidate locations is visualized in figure $\ref{fig:c14_cand_dense}$ as a boolean matrix for which true means a valid traceable candidate and false an invalid tracking location. The subsampled, final output is shown in subfigure $\ref{fig:c14_cand_sparse}$. \\ \\
So, the remaining question is: what step-size should be used to generate the sparse set of tracking candidates? \\ \\
To answer this question we have run different sampling rates on our datasets and visualized the resulting tracking candidates. An example illustrating different sampling rates is dipicted in figure $\ref{fig:sampling_rate_candidates}$. \\ \\
\begin{figure}[H]
\begin{center}
\subfigure[Extreme Oversampling]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/sr_2}
   \label{fig:c14_extreme_oversampling}
}
\subfigure[Oversampling]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/sr_4}
   \label{fig:c14_oversampling}
}
~
\subfigure[Ideal Sampling Rate]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/sr_8}
   \label{fig:c14_ideal_sampling}
}
\subfigure[Undersampling]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/sr_16}
   \label{fig:c14_undersampling}
}
\end{center}
\caption[Density Of Candidates For Different Sampling Rates]{A visualization of different sampling rates of tracking candidates}
\label{fig:sampling_rate_candidates}
\end{figure}
After experimenting with this parameter we observed, that setting the cell size larger than 12 pixels causes a lose in detail since there are not enough points to cover small object parts. On the other hand, cells smaller than 6 pixels waste computation time as smaller objects tend to get smoothed away. Hence, our pipeline samples every 8th pixel by default. However, please not that it is still possible to assign a different value to this parameter. \\ \\
Finally, there is one optional post-processing stage, removing all candidates that would result in looking up a too weak optical flow value. \\ \\
This reduction is computed as the follows: We use the tracking candidate of a frame as lookup coordinates in their corresponding forward flow fields and compute their magnitude. Then, every candidate that maps to a magnitude value smaller than the largest 10 percent of the determined flow magnitudes is discarded. We repeat this procedure for every frame. \\ \\
This drastically reduced the total number of candidates, however may also cancel out useful motion locations. Therefore, this is just optional. Most of the time this removes background pixel locations. An example is shown in figure $\ref{fig:tracable_candidates_strict}$.
\begin{figure}[H]
\begin{center}
\subfigure[All Dense Candidates]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/c14_f1_candidates}
   \label{fig:c14_f1_cand_dense_2}
}
\subfigure[Strict Dense Candidates]{
   \includegraphics[width=0.48\linewidth] {implementation/candidates/strict_dense_candidates}
   \label{fig:c14_cand_dense_strict}
}
\end{center}
\caption[Strict Dense Candidates]{A visualization of the post-processed traceable candidate locations. On the left, the extracted tracking positions, on the right side, an image of the reduced candidates.}
\label{fig:tracable_candidates_strict}
\end{figure}
Both moving objects in the shown example are preserved and only background pixels are removed. However, keep in mind that this post-processing may filter small, meaningful motions. Especially, when there is a lot of camera motion involved, this approach may fail. \\ \\
In the next section we discus how to find occluded regions.

\subsection{Occlusion detection}
In this section we discuss how to determine when we should stop tracking a particular tracking candidate. \\ \\
The previously determined tracking candidates are used as starting points for out point tracking. For every tracking candidate we determine its tracked to position by adding the appropriate forward flow. However, this approach is very error prone because of using incorrect flow estimations or occluded regions. \\ \\
Occlusion may occur whenever a previously visible object gets covered by another object, e.g. a camera captures a scene, during some frames, the observed objects in the background get occluded by another object that is closer to the camera and directly in front of them. \\
So, especially the case of continuing a tracking when a point lands in a occluded regions yields a incorrect tracking. Thus, we have to stop tracking points as soon as those points gets occluded. \\ \\
In tracking tasks, occlusion is usually detected by comparing the appearance of the local neighborhood of the tracked point over time. In contrast, we detect occlusions by verifying the consistency of the forward and the backward flow. In our pipeline such a verification is performed by applying the backward flow $\hat{w}_t$ to a continued tracked point $p_{t+1}$ and checking whether that resulting point has the same origin $\hat{p}_t$ as the actual tracked from position $p_t$. Figure $\ref{fig:occlusion_detection}$ illustrates this concept of occlusion detection.
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.6\linewidth] {implementation/occlusion/occ_det}
\end{center}
\caption[Occlusion Detection]{Conceptual illustration of occlusion detection. For any tracking candidate we apply its corresponding forward flow to find its position in the next frame. From there, we look-up its backward flow and apply it on the tracked to position. If we land close to the point in the original tracked from frame, we say, that the point is not occluded and otherwise we call the point occluded.}
\label{fig:occlusion_detection}
\end{figure}
In the non-occluded case, the backward flow vector $\hat{w}_t$ is supposed to point to the inverse direction of its corresponding forward flow vector $w_t$. Hence, adding $w_t$ to the backward flow vector looked up at the location the forward vector points to should yield approximately the zero vector. If this required consistency is not satisfied, then the point $p_t$ is either getting occluded at frame $t+1$ (the next frame) or the flow was not correctly estimated. Both outcomes are good reasons to stop tracking this point at frame $t$. Since there are always some small estimation errors in the optical flow, there is a small tolerance interval granted. The formula used to decide when a point is not occluded is given in equation $\ref{eq:occlussion_error_tol}$.
\begin{equation}
\begin{aligned}
& \forall p_t \in \text{Frame t}:	\norm{\hat{p_t}-p_t}_2^2 < \epsilon \norm{\hat{w}_t + w_t}_2^2 + b \\
& \text{where } \hat{p_t} = p_{t+1} + \hat{w_t} \text{ with } p_{t+1} = p_t + w_t
\end{aligned}
\label{eq:occlussion_error_tol}
\end{equation} \\
Occlusion comes with the opposite phenomenon disocclision or scaling. To fill these areas not covered by trajectory yet, new trajectories are initialized, in empty areas in each new frame using the same strategy as for the first frame.
\begin{figure}[H]
\begin{center}
\subfigure[Input Image]{
   \includegraphics[width=0.48\linewidth] {implementation/occlusion/01}
   \label{fig:c14_f1_occ}
}
\subfigure[Occluded Regions]{
   \includegraphics[width=0.48\linewidth] {implementation/occlusion/invalid_regsions}
   \label{fig:c14_invalid_regions}
}
\end{center}
\caption[Occluded Regions]{A visualization of the occluded regions. Invalid, occluded regions are colored in white illustrated as shown in subfigure $\ref{fig:c14_invalid_regions}$.}
\label{fig:invalid_regions}
\end{figure}

\subsection{Flow Variance}
\label{sec:flow_variance}
In a later stage in our pipeline, we will compute differences between different optical flow field locations. Since our optical flows may contain noise, especially along motion boundaries, we have to make our pipeline robust to these negative influence of noise. In particular, when taking the difference between two noisy flow locations, the noise level can get amplified. The purpose of this section is to describe a method how to normalize flow differences. 

An simple, but initial first idea would be to smooth the computed flow fields, to reduce the noise level along the boundaries. A commonly used approach to blur an image is to convolve the image with a Gaussian kernel as defined in equation $\ref{eq:gaussian_filter}$.
\begin{equation}
	G_{\sigma}\left( x \right) = \frac{1}{2 \pi \sigma^2} e^{-\frac{x^2}{2 \sigma^2}}
\label{eq:gaussian_filter}
\end{equation}
However, filtering the flows directly is not the best choice. The reason for this is based on the fact that a Gaussian filter acts as a low-pass filter. Thus, after applying this kernel, many details in the flow, such as motion boundaries, will be washed out or will be removed completely. \\ \\
Another blur filter, which addresses the issue of smoothing out edges, is the so called Bilateral Filter $\textbf{BF}$. Similar to the Gaussian Convolution, this filter also computes a weighed average of an image, but the same time takes into account the variation of intensities to preserve edges. 

Let $\bf{I}$ denote an image and $\bf{p}$ one of its pixels. The neighborhood pixels of $\bf{p}$ are denoted by $\mathcal{N}_p$. Then, the Bilateral filter $\textbf{BF}$ can be defined as described in equation $\ref{eq:bilateral_filter}$.
\begin{equation}
	\textbf{BF} \left\{ \bf{I}_{\bf{p}} \right\} = \frac{1}{W_{\bf{p}}} \sum_{\bf{q} \in \mathcal{N}_p} w_{\bf{p}, \bf{q}} \left( \bf{I} \right) \bf{I}_{\bf{q}}
\label{eq:bilateral_filter}
\end{equation}
Note that $w_{\bf{p}, \bf{q}}$ denotes the weight between the pixels $\bf{p}$ and $\bf{q}$, whereas the term $W_{\bf{p}}$ defined in equation $\ref{eq:bilateral_filter_weights}$ is the sum of all weights between a fixed pixel $\bf{p}$ and its neighbors.

\begin{equation}
	w_{\bf{p}, \bf{q}} \left( \bf{I} \right) = G_{\sigma_s} (\norm{\bf{p} - \bf{q}}) G_{\sigma_r} (\bf{I}_{\bf{p}} - \bf{I}_{\bf{q}}) 
\label{eq:bilateral_filter_neighbor_weight}
\end{equation}

The weight in equation $\ref{eq:bilateral_filter_neighbor_weight}$ has a spatial component $G_{\sigma_s} (\norm{\bf{p} - \bf{q}})$ as well as it considers the intensity range $G_{\sigma_r} (\bf{I}_{\bf{p}} - \bf{I}_{\bf{q}})$.

\begin{equation}
	W_{\bf{p}} \left( \bf{I} \right) = \sum_{\bf{q} \in \mathcal{N}_p} w_{\bf{p}, \bf{q}} \left( \bf{I} \right)
\label{eq:bilateral_filter_weights}
\end{equation}
Instead of using a filtered flow field, we go for another, more robust idea. Imagine we would know the error of the computed flows. Then, by using the variance of the field, we directly could address the error by normalizing the flow value by its corresponding variance value. Since we do not know the actual error, we have to define a good estimator of the flow variance. \\ \\
Let $\bf{q}_p$ denote a vector containing all neighbors of pixel $\bf{p}$ and $\bf{w}_p$ is the vector consisting of all weights computed using equation $\ref{eq:bilateral_filter_neighbor_weight}$. Mathematically, this can be formulated as the follows:
\begin{equation}
\begin{aligned}
& \bf{q_p} = \left( q_1, \dots, q_{|\mathcal{N}_p|} \right) \\
& \bf{w_p} = \left( w_{p, q_1}, \dots, w_{p, q_{|\mathcal{N}_p|}} \right) \\
& \text{where } \forall q_k \in \mathcal{N}_p
\end{aligned}
\label{eq:bilat_weight_components}
\end{equation}

The definitions from equation $\ref{eq:bilat_weight_components}$ allow us to define an expected value of pixel $\bf{p}$.

\begin{equation}
	\mathbf{E} \left[ \bf{p} \right] = \frac{\bf{w}_{p}^{T} \bf{q}_p}{\norm{\bf{w}_{p}}}
\end{equation}

For a given optical flow $\bf{F} = (\bf{F_u}, \bf{F_v})$ we can compute its variance by the definition stated in equation $\ref{eq:flow_var_opt_flow}$.
\begin{equation}
	\mathbf{Var} \left[ \bf{F} \right] = \frac{1}{2} \left( \mathbf{Var} \left[ \bf{F_u} \right] + \mathbf{Var} \left[ \bf{F_v} \right] \right)
\label{eq:flow_var_opt_flow}	
\end{equation}

STATE MEANING

\begin{figure}[H]
\begin{center}
\subfigure[Flow Field]{
   \includegraphics[width=0.48\linewidth] {implementation/flow_var/ff_c14_1}
   \label{fig:c14_fv_flow}
}
\subfigure[Variance Field]{
   \includegraphics[width=0.48\linewidth] {implementation/flow_var/fv_c14_1}
   \label{fig:c14_fv_var}
}
\end{center}
\caption[Flow Variance]{Visualization of the flow variance computed by applying a bilateral filter on the optical forward flow.}
\label{fig:flow_variance}
\end{figure}


flow estimation can be noisy for various reasons and the problem is that the nois is often proportional to the magnitude of the absolud flow. thus using a simple flow difference as ameasure will constantly underestimate the simmilarity between heigh magnitude flow. 

Lets assume 

\begin{equation}
\begin{aligned}
\colvec{u_1}{v_1} = f + \colvec{x_1}{y_1} \\
\colvec{u_2}{v_2} = f + \colvec{x_2}{y_2}
\end{aligned}
\label{eq:def_flow_tracking}	
\end{equation}

where f is the true flow and 
\begin{equation}
	\colvec{x_1}{y_1} \sim \colvec{x_2}{y_2}
\end{equation}

Let us assume, that the random variables $x_1$ and $x_2$ are i.i.d. distributed having a zero mean and a variance $\sigma_x^2$, $x_1$ and $x_2$ respectively, i.e.

\begin{equation}
\begin{aligned}
x_1 \sim x_2 (0, \sigma_x^2) \\
y_1 \sim y_2 (0, \sigma_y^2
\end{aligned}
\label{eq:def_flow_tracking}	
\end{equation}

\begin{equation}
\begin{aligned}
\mathbf{E} \left[ \norm{\colvec{u_1}{v_1} - \colvec{u_2}{v_2}} \right]
& = \mathbf{E} \left[ (u_1 - u_2)^2 \right] + \mathbf{E} \left[ (v_1 - v_2)^2 \right] \\
& = \mathbf{E} \left[ u_1^2 - 2 u_1 u_2 + u_2^2 \right] + \mathbf{E} \left[ v_1^2 - 2 v_1 v_2 + v_2^2 \right] \\
& = \mathbf{E} \left[ x_1^2 \right] + \mathbf{E} \left[ x_2^2 \right] + \mathbf{E} \left[ y_1^2 \right] + \mathbf{E} \left[ y_2^2 \right] \\
& = 2 \left( \sigma_x^2 + \sigma_y^2 \right)
\end{aligned}
\label{eq:flow_variance_formula}	
\end{equation}
In the last step of equation $\ref{eq:flow_variance_formula}$ we used the abbreviation $\sigma_x^2$ which denotes the variance of the optical flow along its $x$ direction. In this derivation we exploited the fact that $x_1$ and $x_2$ are independent random variables, $y_1$ and $y_2$ respectively. Hence, the term $\mathbf{E} \left[ x_1 x_2\right]$ yields the value zero. \\ \\
We experimentally also have defined an expression for computing depth variances. However, we this special run-mode has only been implemented to demonstrate the limitations of our pipeline. Therefore, the description of this procedure can be found in the appendix $\ref{sec:depth_field_variances}$ on page $\pageref{sec:depth_field_variances}$.

\section{Forming Trajectories}
As discussed earlier, motion is a spatially and temporal coherent visual cue. This key insight tells us that motion is not frame-wise independent. Hence, we have to extract the whole motion history of every frame location and feed it to a particular segmentation method in order to produce viable motion groupings. To build the history of a point we track it to its successor frames using the optical flow. Such a ordered list of tracked to points is called trajectory. Finally, the set of all possible trajectories forms the seeked motion history. In this section we explain in detail the required steps to generate such trajectories and their 3d version using camera calibration data.

\subsection{Point tracking}
Every point in frame $t$ can be tracked to the next frame $t+1$ by using the optical flow field $w_t$ that belongs to frame $t$. 	In principle, any optical flow method can be used. Our implementation supports various flow methods$\footnote{In general, our pipeline supports any flow method that can produce .flo files. However, the follwoing methods are directly supported: the \textit{ldof}, \textit{hs}, \textit{srsf} and \textit{lrgbd} flow methods}$. Please refer to section $\ref{sec:impl_optical_flow}$ on page $\pageref{sec:impl_optical_flow}$ to get a detailed list and description. The algorithm $\ref{alg:point_tracking}$ describes the required step to perform the points tracking. \\ \\
\begin{algorithm}[H]
\caption{Point Tracking}
\begin{table}[H]
  \begin{tabular}{@{}lll@{}}
    \textbf{Input:} & Tracking candidates $\textbf{TC}$ \\
    	& Forward Flow fields $\textbf{F}$ \\
        & Occluded Regions $\textbf{OR}$ \\
	\textbf{Output:} & Trajectories $\textbf{TC}$
  \end{tabular} 
\end{table}
\setlength{\fboxrule}{0pt} 
\begin{boxedminipage}{1.0\textwidth}
  \begin{algorithmic}[1]
  	\State $\text{Load tracking candidates TC of first frame} f_1$
  	\State $\text{Load forward flow } \text{ff}_1 \text{ of first frame } f_1.$
  	\State $\text{Initalize a PointList } \bf{PL} \text{ containing all tracked to positions}.$
  	\ForAll{$\text{trackingCandidate.position } p \in TC\left( f_1 \right)$}
  		\State $\text{Fetch flow vector fv} = \textbf{F} \left( f_1, p \right)$
  		\State $\text{Compute the tracked to position: } tp = p + fv$
  		\State $\text{Append tp to TC}$
    \EndFor
    \State $\text{Append TC to the Tracking Candidates that belong to Frame 2}.$
  	\ForAll{$\text{Frame } f \in \{ \text{Frame}_2,\dots, \text{Frame}_n\}$}
  	  	\State $\text{Load tracking candidates TC of first frame} f$
  		\State $\text{Load forward flow } \text{ff} \text{ of frame } f.$
  		\State $\text{Initalize a PointList } \bf{PL} \text{ containing all tracked to positions}.$
  		\ForAll{$\text{trackingCandidate.position } p \in TC\left( f \right)$}
  			\State $\text{Fetch flow vector fv} = \textbf{F} \left( f, p \right)$
  			\State $\text{Compute the tracked to position: } tp = p + fv$
  			\State $\text{Append tp to TC}$
    	\EndFor
    	\State $\text{Append TC to the Tracking Candidates that belong to next frame}.$
    \EndFor
  \end{algorithmic}
  \end{boxedminipage}
  \vskip1.5pt
\label{alg:point_tracking}
\end{algorithm}
$\newline$
Initially, we load all the extracted Harris corner feature locations that belong to the first dataset frame and its corresponding forward flow field. For each such feature location, we lookup its value in the flow field. Next, we add this looked-up flow vector to currently considered feature image coordinate. The result is the \textit{tracked to} position within the second frame. If this computed tracked to position is not occluded, we keep it. Otherwise we discard it. Deciding whether or not a location is occluded is done by performing a lookup in the occlusion mask. Moreover, points that land outside valid image locations are discarded. \\ \\
After processing the first frame, we continue with tracking points in the next frame. This time we not only have to track the extracted Harris features, but we also have to continue every \textit{tracked to} position from the previous iteration. To fill disoccluded areas not covered by trajectory yet, new trajectories are initialized, in empty areas in each new frame using the same strategy as for the first frame. \\ \\
The set of all these locations is processed the same way as described as the points in the initial stage. We repeat this procedure until we visited every dataset frame. \\ \\
Again, please note that tracking data has to be stopped as soon as a point gets occluded. otherwise, the point trajectory will share the motion of two different objects. \\ \\
Finally, we conclude this section by offering an example of an actual point tracking applied on the cars dataset. The resulting trajectories, as well as their tracking points, are shown in figure $\ref{fig:cars_trajectories}$. In this visualization we see different trajectories, tracked over four frames. Tracking points that belong to the same trajectory have the same color as their trajectory. As we can see, points that were tracked in the same moving objects exhibit coherent and consistent trajectories. In other words, our flow field based tracker produces reliable and meaningful results in this example.
\begin{figure}[H]
\begin{center}

\includegraphics[width=0.65\linewidth] {implementation/trajectories/cars_trajectories_4_sel}
\end{center}
\caption[Trajectories]{Exemplary trajectories tracked in the cars dataset shown in different color. The tracking points exhibit the same color as the trajectory the belong to. The tracking points are plotted as thick dots.}
\label{fig:cars_trajectories}
\end{figure}

\subsection{Transformation to 3d points}
\label{subsection:transform_to_3d_points}
Ultimately, our pipeline should be able to work with 3d data. To enable this, we have to make use of the extracted depth map and use their information in order to transform the 2d trajectory points to 3d points in space. Ideally, the distances, computed to define an affinity measure between trajectories, should be more accurate. 

Given the principal point $p = \left(p_x, p_y \right)$ and the focal lengths $\left(f_x, f_y \right)$ we can compute the following homogeneous transformation:
\begin{equation}
\colvec[f_x X + Z p_x]{f_y Y + Z p_y}{Z} =
\begin{bmatrix}
f_x & 0 & p_x & 0 \\
0 & f_y & p_y & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
\begin{pmatrix}
X \\
Y \\
Z \\
1
\end{pmatrix}
\end{equation}

In the following, let us assume $d \equiv z$. Then

\begin{equation}
	\colvec[X]{X}{Z} \mapsto \left( \frac{f_x x}{d} + p_x, \frac{f_y y}{d} + p_y \right)^T
\end{equation}

since

\begin{equation}
\begin{aligned}
	d \left( \frac{x - p_x}{f_x} \right) = \hat{x} \Leftrightarrow \frac{\hat{x}}{d} f_x + p_x = x \\
	d \left( \frac{y - p_y}{f_y} \right) = \hat{y} \Leftrightarrow \frac{\hat{y} f_y}{d} + p_y = y 
\end{aligned}
\label{eq:depth_tranfomation}
\end{equation}.
For given pixel coorinates $\left( x,y \right)$ that indicate image locations, we want to transform them into camera coordinates, using depth maps, by applying equation $\ref{eq:depth_tranfomation}$. \\ \\

Given the extrinsic and intrinsic camera calibration data,
\begin{equation}
\begin{aligned}
	\text{Camera}^{depth} : \left( p^d = (p_x^d, p_y^d), f^d = (f_x^d, f_y^d)\right) \\
	\text{Camera}^{color} : \left( p^c = (p_x^c, p_y^c), f^c = (f_x^c, f_y^c)\right) \\
	\textbf{E} : \text{Camera}^{depth} \rightarrow \text{Camera}^{color}
\end{aligned}
\label{eq:calib_data}
\end{equation}.
Steps to transform 2d image locations $\left( u, v \right)$ to 3d positions $\left( x, y, z \right)$ using the camera calibration data defined as in equation $\ref{eq:calib_data}$. Furthermore, let the depth value d $\in$ $\bf{D}$. Then we have to perform the following transformations steps:
\begin{enumerate}
\item Transform image location onto depth camera space
\begin{equation}
	\forall (u, v), \forall d = \textbf{D}(u,v): \colvec[\frac{u - p_x^d}{f_x^d} d]{\frac{v - p_y^d}{f_y^d} d}{d} = \colvec[x]{y}{z} \equiv \bf{p}
\end{equation}
\item Align depth camera on color camera by appling the extrinsic matrix $E$:
\begin{equation}
	\forall p: \hat{p} \equiv \colvec[\hat{x}]{\hat{y}}{\hat{z}} =  E p
\end{equation}
\item transform onto color camera
\begin{equation}
	\forall \hat{p} : \left( \hat{u}, \hat{v}\right) = \colvec{\frac{\hat{x}}{\hat{z}} f_x^c + p_x^c}{\frac{\hat{y}}{\hat{z}} f_y^c + p_y^c}
\end{equation}
\end{enumerate}

\section{Affinity Matrix}
\label{sec:affinity_matrix_impl}
In this section we describe how we compute the so called affinity matrix, which is later used to compute the actual motion segmentation. \\ \\
Generally, trajectories of longer videos are asynchronous, i.e. they cover different temporal windows in a shot. Furthermore, the number of points that are tracked across the whole image sequence is small or even empty due to occlusion, disocclusion or the sampling rate. Hence, a measurement matrix that utilizes the coordinates of the tracked points, as used by the multi-body factorization and subspace methods, will have many missing entries and thus is a bad choice. \\ \\
However, there is a remedy to this issue. We instead compute pairwise affinities between trajectories, because this only requires some trajectories to have some temporal overlap. We define affinities between all pairs of trajectories that share at least a common frame. In the following we denote the tracking points in these shared frames as overlapping parts. \\ \\
According to the gestalt principle of common fate, we should assign high affinities to pairs of points that move together. \\
Imagine the situation where two persons are walking next to each other$\footnote{Assume both persons are walking at the same speed in the same direction, ignoring all motions due to deformation.}$. Although they are different objects, both of them share the same motion. Moreover, a not moving person that sits in a chair shares the same motion as the objects in the background. The gestalt principle tells us that these situations should be treated conservatively and the objects from these examples should not be separated. \\ \\
In the following we describe in detail the steps of algorithm $\ref{alg:computing_affinity_matrix}$ which is used to generate the affinity matrix.
\begin{algorithm}[H]
\caption{Generating Affinity Matrix}
\begin{table}[H]
  \begin{tabular}{@{}lll@{}}
    \textbf{Input:} & Extracted Trajectories \bf{T} \\
		& Forward Flows \bf{OF} \\
 		& Dataset Images (\emph{optional}) \bf{F} \\
    \textbf{Output:} & Affinity Matrix \bf{W} \\
    & Nearest Trajectory Neighbors \bf{$\mathcal{N}$}\\
    & Trajectory Labels \bf{$\mathcal{L}$}\\
  \end{tabular} 
\end{table}
\setlength{\fboxrule}{0pt} 
\begin{boxedminipage}{1.0\textwidth}
  \begin{algorithmic}[1]
      \State $\text{Form every possible trajectory pair combination of } \bf{T}$
      \State $\text{Filter too short trajectories}$
      \ForAll{$\text{Trajectory pair } \left( \textbf{a}, \textbf{b} \right)$}
        \State $\left( \textbf{p}^a, \textbf{p}^b \right) \longleftarrow\text{Select all temporal overlapping segments between } \textbf{a} \text{ and } \textbf{b}.$
        \State $d_{\text{spatial}} = \text{computeSpatialDistance}\left( \textbf{p}^a, \textbf{p}^b, \textbf{T} \right)$
        \State $d_{\text{color}} = \text{computeColorDistance}\left( \textbf{p}^a, \textbf{p}^b, \textbf{F} \right)$
        \State $d_{\text{motion}} = \text{computeMotionDistance}\left( \textbf{p}^a, \textbf{p}^b, \textbf{F} \right)$
        \State $d^2 = \text{combineDistances} \left( d_{\text{spatial}}, d_{\text{color}}, d_{\text{motion}} \right)$
        \State $W \left( \textbf{a}\text{.label}, \textbf{b}\text{.label} \right) = e^{-\lambda d^2}$
        \State $\text{Update nearest neighbors of } \textbf{a} \text{ and } \textbf{b} \text{ using } d_{\text{spatial}}$
      \EndFor
  \end{algorithmic}
  \end{boxedminipage}
  \vskip1.5pt
\label{alg:computing_affinity_matrix}
\end{algorithm}
As the very first step, our pipeline loads the previously extracted trajectories into its memory. Each loaded trajectory gets a unique identifier, which is called $\textit{label}$, assigned. Next, every possible trajectory pair combination is formed. Trajectories that are shorter than a user specified are filtered and thus discarded. \\ \\
For each remaining trajectory pair, different distance values are computed and combined according to an initially by a pre-specified run-mode. More precisely, our pipeline computes the following three distances between temporal overlapping trajectory segments$\footnote{By the term \textit{temporal overlapping} we refer to tracking point in a trajectory pair that share a common frame.}$:
\begin{itemize}
  \item The \textbf{spatial distance} $d_{\text{spatial}}$: Trajectories that belong to the same object are usually spatially close to each other.
  \item The \textbf{color distance} $d_{\text{color}}$: Within a certain object region, associated trajectories are tracked at locations that have a similar color value.
  \item The \textbf{motion distance} $d_{\text{motion}}$: according to the gestalt principle we can conclude, that for each pair of trajectories at the time instant where the motion is maximally different, the two points do not belong to the same object.
\end{itemize}
So the questions raise: how are these distances computed and how are they combined? \\ \\
The spatial distance is simply equals to the average distance between the overlapping pair segments according to the euclidean norm as defined in equation $\ref{eq:spatial_distance}$.\\ \\
For computing the color distance we use the definition of equation $\ref{eq:color_dist}$. For every overlapping tracking point of any pair, we lookup the color value in its corresponding frame. Then, we compute the distance between the looked-up color values by transforming the color values to the $\text{CIE l*a*b}$ colorspace$\footnote{This transformation enables us to use the l2 norm in order to compute color distances.}$ and then applying the l2 norm. We finally compute the average of these color distances which is equals $d_{\text{color}}$. \\ \\
Lastly, some details about how to compute the motion distance which is formulated in equation $\ref{eq:motion_distance}$. For both trajectories of any pair, we iterate over their overlapping tracking points. The coordinates of those points are used as lookup indices in the corresponding forward flow fields. The idea is to compute the forward difference for every lookup point with a certain step size similar as shown in figure $\ref{fig:motion_dist}$. These forward differences (blue and green arrow) are then assigned to every tracking point. Next, we compute the euclidean norm between the overlapping forward differences. The final motion distance is the average of all these forward difference norms. Furthermore, please notice that the resulting motion distance is normalized by its corresponding flow variance value. Further information about this normalization, as well as about the definition of this variance can be found in section $\ref{sec:flow_variance}$ on page $\pageref{sec:flow_variance}$.
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth] {implementation/affinities/motion_dist}
   \label{fig:cars_w}
\end{center}
\caption[Motion Distance]{Illustration how the computed forward differences look like indicated by the blue and green vectors. These differences are then used for computing the motion distance $d_{\text{motion}}$.}
\label{fig:motion_dist}
\end{figure}
There are two main setting a user has to specify preliminarily. Whether or not depth cues should be used and which kind of similarity measure should be used. Remember that the selected similarity measure determines how the computed distances should be combined. Both options have a large impact on the final outcome of the affinity matrix. \\ \\
When enabling depth cues, all available depth fields are loaded into the pipeline. In combination with the depth- and color-camera calibration data, this enables the pipeline to compute the three dimensional version of each traced trajectory point, analogous as described in the previous subsection $\ref{subsection:transform_to_3d_points}$ on page $\pageref{subsection:transform_to_3d_points}$. This 3d positions are then used for computing the spatial distance between the overlapping parts of any trajectory pair. \\ \\
Our pipeline basically supports two distance combination methods: Either it combines them by calculating the weighted sum of the distance values or it multiplies all the distances. Depending on the used combination strategy, different segmentation methods will later be used. Summed distances can only be used in combination together with the Kernighan-Lin segmentation method whereas the product of distances can be either used in our spectral clustering- or min-cut segmentation method$\footnote{The reason for this limitation is due to the fact that the summed distance method may yield negative affinities since some weights are negative.}$.
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth] {implementation/affinities/modes}
   \label{fig:cars_w}
\end{center}
\caption[Affinity Pipeline Modes]{An visualization of all affinity computation modes that can be run by our pipeline. Abbreviations \textbf{PD} and \textbf{PED} stand for product of distances, whereas the additional \it{E} in PED stands for euclidean and hints that we are using Euclidian, metric units instead pixel units. Similarly, \textbf{SD} and \textbf{SED} stand for summed (euclidean) distances. Please note that affinity matrices generated by the mode SD or SED can only be used in combination with the \textit{Kernighan-Lin} segmentation method but affinities generated by running the modes PD and PED can be used together with spectral clustering and min-cut.}
\label{fig:affinity_modes}
\end{figure}
As shown in figure $\ref{fig:affinity_modes}$, our pipeline offers four different affinity generating modes: PD, SD, PED and SED. \\ \\ 
When running the mode PD or PED we combine distance by multiplying them. The affinity matrix is computed according to the definition of equation $\ref{eq:prod_dist_affinity}$, the negative weighted exponential of the combined distance value. Please note that this combination function always yields values between zero and one. A value close to one means, that the two trajectories are very similar, whereas a value close to zero or even equals zero means they are very distinct according to the used metrics. \\ \\
However, when running the mode SD or SED the distances are summed and the corresponding affinity is computed as defined in equation $\ref{eq:sum_dist}$. This combination function yields arbitrary values in the real numbers. This combination strategy returns an affinity measure that is dedicated to the Kernighan-Lin segmentation implementation. \\ \\
Additionally, our pipeline also offers a mode to compute affinities between shortly disconnected trajectories. The idea is to prepend (extend) one additional tracking point to a predecessor (successor) frame by adding the average motion distance of the next (previous) five frames to the trajectory's starting point (end point). Please note that this run-mode is only is optional and thus has to be invoked manually. Moreover, we could not observe any improvements in the resulting segmentations, when enabling this mode. \\ \\ 
In summary, the affinity matrix is computed between a trajectory and every other trajectory by calculating the distances described above and then combining them in a certain way. Every trajectory instance therefore carries a list of affinities, which are sorted by their partner trajectory's label in an ascending order. \\ \\
Therefore, for building the affinity matrix, we fetch all used trajectories, order them by their label value (again, ascending). Then, all we have to do is to traverse these ordered trajectories and then write their ordered affinities (to the other trajectories) into a file. Actually, each trajectory forms a line, where a line consists of the affinity values of the currently visited trajectory. Our writer uses a special file formatter that is able to generate a Matlab readable mat file. The pairwise affinities for $n$ valid trajectories result in an $n \times n$ affinity matrix $W$ as shown in figure $\ref{fig:cars_affinity_matrix}$. \\ \\
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.65\linewidth] {implementation/affinities/cars/cars_w}
   \label{fig:cars_w}
\end{center}
\caption[Affinity Matrix]{Visualization of affinities among trajectories and the structure of the matrix $W$.}
\label{fig:cars_affinity_matrix}
\end{figure}
To later support queries that allow to fetch affinities by a trajectory label, we also have to write a file containing every valid trajectory label identifiers. The labels are in an ascending order with respect to their label id. Figure $\ref{fig:affinity_index_label_mapping}$ shows how an affinity lookup by two given trajectory labels can be performed.
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.85\linewidth] {implementation/affinities/affinity_label_mapping}
   \label{fig:cars_w}
\end{center}
\caption[Affinity Matrix Index Trajectory Label Mapping]{To lookup an affinity value between two trajectories we first have to lookup their matrix indices. This is performed by using the trajectory label / affinity matrix id mapping. In this example, we want to get the affinities between the trajectories with id 18 and 13. We first look those label's matrix indices and then use those indices to lookup the actual affinity value.}
\label{fig:affinity_index_label_mapping}
\end{figure}
Apart from generating the affinity matrix our pipeline also extracts the per trajectory nearest neighboring trajectories. The distance to those neighbors is measured with respect to the average spatial distance. Luckily, we already have computed this spatial distance value while traversing the trajectory pairs. \\ \\
To conclude this section and offer the reader a better understanding what those affinities actually are and mean, we discuss an affinity visualization, produced by our pipeline, shown in figure $\ref{fig:cars_affinities}$.
\begin{figure}[H]
\begin{center}
\subfigure[Similarities Car Foreground]{
   \includegraphics[width=0.48\linewidth] {implementation/affinities/cars/fc}
   \label{fig:cars_a}
}
\subfigure[Similarities Car Background]{
   \includegraphics[width=0.48\linewidth] {implementation/affinities/cars/cb}
   \label{fig:cars_b}
}
~
\subfigure[Similarities Woods]{
   \includegraphics[width=0.48\linewidth] {implementation/affinities/cars/woods}
   \label{fig:cars_c}
}
\subfigure[Similarities Street]{
   \includegraphics[width=0.48\linewidth] {implementation/affinities/cars/street}
   \label{fig:cars_d}
}
\end{center}
\caption[Trajectory Affinities]{Visualization of affinities among trajectories and the structure of the matrix $W$.}
\label{fig:cars_affinities}
\end{figure}
In this figure, we visualize four examples of the affinity values between a selected point (marked by a red dot) and every other trajectory. The larger the similarity, the more yellow the color becomes and, vice versa,  the smaller the affinity, the more blue is becomes. For example in subfigure $\ref{fig:cars_b}$, we selected a point on the moving car in the background. It has strong affinities to tracked points within the car and small to the front car and to the background. Similar results are shown in the other subfigures. This results hints the potential of our pipeline. \\ \\
In the next section we will discuss methods that use the previously generated output to generate a sparse motion segmentation.
\section{Sparse Motion Segmentation}
In this section we explain how we generate sparse motion segmentations using the previously calculated affinity matrix. \\ \\
In fact, our pipeline supports three different segmentation methods, all having their pros and contras. In particular can can perform the task of motion segmentation using spectral clustering of the affinity matrix (SC), building a graph of the affinity matrix and calculating several cuts via min-cut (MC) or lastly, by using the affinities to solve the graph partitioning problem using the Kernighan-Lin Heuristic (KL). \\ \\
Every implemented segmentation method returns a file containing a mapping between the trajectory label and their segment label. This allows to draw motion segmentations resulting by our segmentation methods. \\ \\
In order to draw the segmentation of the $k-th$ frame in a certain dataset, we load all trajectories that are active in that frame. For a trajectory, being active in a certain frame refers to a trajectory which contains a point that was tracked in that particular frame. To draw a segmentation pixel, the determined segment label defines the color and the tracking point its position in the corresponding frame. \\ \\
In the following we offer a detailed explanation of these motion segmentation methods.
\subsection{Spectral Clustering}
In the previous section we described how the pairwise affinities can be computed. For $n$ trajectories we ended up by a a $n \times n$ affinity matrix $W$. \\ \\ 
One possible way to interpret these matrix entries is to identify them as edge weights of a fully connected graph $G$, where each trajectory represents a node. Following this interpretation, then, the task of extracting the moving objects  corresponds to the task of grouping several similar graph vertices. \\ \\
An approximately optimal partitioning of $G$ can be obtained via spectral clustering. The idea behind this approach is to decompose a transformed version of W and then cluster only a few of the resulting components to obtain the moving objects. In the following we provide a detailed description how to perform such a clustering. \\ \\
As the first step, we have to transform the affinity matrix $W$ in a special way to obtain the normalized graph Laplacian $L$. The exact definition of this transformation is given by equation $\ref{eq:def_l_mat}$.
\begin{equation}
	L = D^{-\frac{1}{2}} \left( D - W \right) D^{-\frac{1}{2}}
\label{eq:def_l_mat}
\end{equation}
$D$ denotes a $n \times n$ diagonal matrix, where the $n-th$ diagonal entry corresponds to the summed elements in W of its $n-th$ row. An exact definition of D is given by equation $\ref{eq:def_d_mat}$. \\ \\
Next, we perform an eigenvalue decomposition on the Laplacian matrix $L$ as defined in equation $\ref{eq:eigenvalue_decomposition_l}$.
\begin{equation}
	V^{T} \Lambda V = L
\label{eq:eigenvalue_decomposition_l}
\end{equation}
The decomposition yields the matrix $V$, which contains the row-wise eigenvectors of $L$ and the diagonal matrix $\Lambda$, which contains the eigenvalues. \\ \\
The actual clustering is performed on the eigenvectors of $L$. In fact, we further assume that we only need some of the eigenvectors for the clustering. Therefore, we chose those eigenvectors that correspond to the $m$ smallest eigenvectors $\lambda \in \Lambda$. \\ \\
Our clustering method implements a k-means algorithm that runs on the given eigenvectors, where k corresponds to a pre-specified number of clusters. To ensure numerical stability, we initially normalize the given eigenvectors to the range $\left[ 0,1 \right]$. \\ \\
The output of this method is a grouping of vector indices. Please notice that all indices, which belong to the same group, correspond to the same moving object. In particular, it is easily possible to map the group indices to actual trajectories by making use of the label mapping list. This list has been computed in the previous pipeline stage. The mentioned mapping is illustrated in figure $\ref{fig:affinity_index_label_mapping}$. \\ \\
One important aside about the eigen-decomposition step: Usually, the number of objects is expected to be significant smaller than the number of trajectories. Therefore, an efficient iterative method for computing the eigenvectors and eigenvalues can be used and the whole computation is feasible. \\ \\
In the following, let us discuss how many eigenvectors should be selected, respectively, what value $m$ should correspond to. \\ \\
We filter all trivial solutions. Those are the eigenvectors that correspond to an eigenvalue equals zero. In our implementation basically allows to manually specify the number of eigenvectors that should be used (i.e. the smallest m ) or to use the default mode, which selects twice as many eigenvectors as the cluster count we want to solve for. We also experimented with finding the optimal value for $m$ by generating all possible segmentations for m equals two to two times the cluster count for a fixed cluster count. Then, by comparing the resulting segmentations against ground truth images, we selected the best assignment for m. However, such a approx yields dataset-dependent values for m. Furthermore, it assumes the presence of ground truth frames. Lastly, this is very time consuming. Therefore, we decided to use the default mode, using twice the cluster count for m. \\ \\
Analogously, we could ask ourself: what is the best value for the cluster count. Again, we generate all possible segmentations for a fixed value of m by varying the cluster count. However, we found a good heuristic, by using twice the number of expected moving objects. For further information please refer to the chapter results. \\ \\
The limitation of this approach is that such a clustering is only defined for non-negative affinity values between trajectories. Otherwise the eigenvalue decomposition of the Laplacian would yield complex valued eigenvectors. In practise, this assumption implies that it is easy to specify which trajectories should be in the same segment but it is impossible to impose that they should be separated. \\ \\
Spectral clustering is supposed to produce oversegmentations and there is not always smooth transitions between the eigenvectors.
\subsection{MinCut} 
In this section we describe an alternative method to generate motions segmentation. This approach directly addresses some issues of the previously described spectral clustering approach. \\ \\
Generally, eigenvectors typically show smooth transitions within a region and more or less clear edges between regions. Unfortunately, the k-means algorithm cannot properly deal with this setting. Its results generally suffer from oversegmentations since it approximates smooth transitions by multiple constant functions. Furthermore, the optimal number of clusters $K$ is by no means obvious, because clusters are represented by many eigenvectors. \\ \\
To address this issue, we formulate an energy, similar as in $\cite{Bro10c}$, that comprises a spatial regularity term. By minimizing this energy term we obtain the optimal segmentation assignment. \\ \\
Initially, we again load the affinity matrix $W$, but this time together with the extracted trajectory's nearest neighbors $\mathcal{N}$. Analogously as we did in the spectral clustering section, we transform $W$ to the normalized Laplacian and decompose it into its eigenvalues and eigenvectors. We apply the exact same strategy for choosing an appropriate value for $m$ as well as for the cluster count we want to solve for, as we described previously. \\ \\
In the following we describe this energy term, but before doing so, we have to provide the reader some definitions. Let $v_i^a$ denote the $a$-th component of the $i$-th eigenvector and $\textbf{v}^a$ the vector compose of the $a$-th components of all m extracted eigenvalues. Notice that the index a maps to a distinct trajectory. Let $\mathcal{N} \left( a \right)$ denote the neighboring trajectories of trajectory a, which were initially loaded. For a fixed number of clusters $K$, we want to compute the optimal trajectory assignments $\pi^{a} \in \{ 1, \dots , K \}$ such that the energy formulated in equation $\ref{eq:min_cut_energy_revisited}$ is minimized.
\begin{equation}
\begin{aligned}
& E \left( \pi, K \right) = \underbrace{\sum_{A} \sum_{k=1}^K \delta_{\pi^A, k} \norm{\bf{v}^A - \mu_k}_{\lambda}}_\text{data term} + \underbrace{\nu \sum_A \sum_{B \in \mathcal{N}(A)} \frac{1- \delta_{\pi^A, \pi^B}}{\norm{\bf{v}^A - \bf{v}^B}}}_\text{smoothness term} \\
& \text{where } \norm{\bf{v}^A - \bf{\mu}}_\lambda = \sum \frac{v_i^A -  \mu_i}{\lambda_i}
\end{aligned} 
\label{eq:min_cut_energy_revisited}
\end{equation}
The $\delta_{\pi^A, \pi^B}$ is a characteristic function which is equals one if the trajectories $A$ and $B$ are assigned to the same cluster and zero otherwise. Similarly, the expression $\delta_{\pi^A, k}$ is a boolean function that is one, if the trajectory $A$ got the label k assigned. \\ \\
The $\textbf{data term}$ models an unary cost that is minimized by k-means, where $\mu_k$ denotes the centroid of cluster $k$. Notice that this term makes use of the norm $\norm{.}_{\lambda}$. In this norm each eigenvector is weighted by the inverse of the square root of its corresponding eigenvalue. Such a weighting ensures that eigenvectors that separate more distinct clusters correspond to smaller eigenvalues. \\ \\
The $\textbf{smoothness term}$ acts as a regularizer that penalizes the spatial boundaries between clusters. The penalty is weighted by the inverse difference of the eigenvectors along these boundaries. On one hand, this penalty is very small for the case when there are clear discontinuities along cluster boundaries. On the other hand, boundaries within a smooth area are penalized for more heavily. This avoids splitting clusters at arbitrary locations due to smooth transitions in the eigenvectors. \\ \\
The parameter $\nu$ acts as the tread-off between the two terms and thus determines how smooth the final segmentation should be. \\ \\
Minimizing the energy formulated energy in equation $\ref{eq:min_cut_energy_revisited}$ is a difficult problem, because it exhibit many local minima. However, when using a fixed number of clusters $K$, then minimizing this energy becomes a multi-label Markov Random Field (MRF) optimization problem with unknown centroids. \\ \\
In our implementation we optimize this minimization problem by using an alternating iterative solver. Initially, we assign an equidistant labeling. The precise definition of this initialization is formulated in equation $\ref{eq:initialization_min_cut}$.
\begin{equation}
	\forall k \in \{ 1, \dots, K \} \forall A_i : (k - 1) \frac{n}{K} \leq i \leq k \frac{n}{K} : \pi^{A_i} = k
\label{eq:initialization_min_cut}
\end{equation}
In the first pass, we fix the label assignments and solve for the cluster centroids by running k-means. In the next pass, we fix the centroids using the k-means result and optimize for the assignments by running a multi-label graph cut energy minimization. For this purpose we rely on $\cite{Fulkerson2009}$'s $\textit{GCMex}$ framework, which is a freely available Matlab implementation, that can perform the task of generating multi-label graph cut. We repeat this procedure until the labels assignments have converged. The resulting assignment is the motion segmentation.

\subsection{Kernighan-Lin} 
formulate trajectory motion segmentation as a minimum cost multi-cut problem on a trajectory graph. In this graph, positive and negative edge weights are computed from motion, position and color clues. positive and negative weights allow to optimize for the correct number of moving objects including small objects that show discriminative motion in a small number of frames. this approach is supposed to outperform spectral clustering as showing in $\cite{KB15b}$. \\ \\
Let $G = (E, V)$ be an undirected graph, where $V$ is the set of vertices and $E$ the set of edges. The $KL$ algorithm attempts to find a partition of V into Two disjoint subsets $A$ and $B$ of equal size, such that the sum $T$ of edge weights between the vertices in $A$ and $B$ is minimized. For defining the cost, let $I_a$ be the internal cost of the vertex $a$, which is defined as the sum of the costs of the edges between $a$ and other nodes in $A$. Furthermore, let $E_a$ denote the external cost of $a$, which is defined as the sum of the costs of edges between $a$ and the vertices in $B$. The balance of a vertex is given by the difference defined in equation $\ref{eq:kl_d_values}$.
\begin{equation}
	D_a = E_a - I_a
\label{eq:kl_d_values}
\end{equation}
If $a$ and $b$ are interchanged, then the reduction in cost is equals.
\begin{equation}
	T_{n} - T_{n-1} = D_a + D_b - 2c_{a,b}
\label{eq:kl_difference_ext_int_costs}
\end{equation}
Note that $c_{a,b}$ in equation $\ref{eq:kl_difference_ext_int_costs}$ defines the cost of the possible edge between $a$ and $b$. \\ \\
The $KL$ algorithms attempts to find an optimal series of interchange operations between the elements in $A$ and $B$, which maximizes the value of equation $\ref{eq:kl_difference_ext_int_costs}$. The determined optimal operation sequence is then executed to compute the partition of the graph into the sets $A$ and $B$. \\ \\
In the following the pseudo-code of \emph{Kernighan-Lin} algorithm we used for our implemented, listed in algorithm $\ref{alg:kernighan_lin}$.
\begin{algorithm}[H]
\caption{Kernighan-Lin}
\begin{table}[H]
  \begin{tabular}{@{}lll@{}}
    \textbf{Input:} & Graph \emph{G = (V, E)} \\
    \textbf{Output:} & Binary Graph Partition $\left( A, B \right)$ \\
  \end{tabular} 
\end{table}
\setlength{\fboxrule}{0pt} 
\begin{boxedminipage}{1.0\textwidth}
  \begin{algorithmic}[1]
  	  \State $\text{Determine initial balanced vertex partition into sets A, B}$
      \Do
		\State $\forall a \in A, \forall b \in B: \text{Compute D values}$
		\State $\text{Let gv, av, bv be empty lists}$
		\For{$\text{n=1 to } |V|/2$}
			\State $\text{Find } a \in A, b \in B \text{ such that } g = D_a + D_b - 2e_{a,b} \text{ is maximal}$
			\State $\text{Remove a and b from further consideration in this pass}$
			\State $\text{Put g to gv, a to av, b to bv}$
			\State $\text{Update D values for all elements of } A = A \backslash \{a\}, B = B \backslash \{b\}$
		\EndFor
		\State $\text{Find k that maximizes } g_{max} = \sum_{i=1}^k gv_i$
		\If{$g_{max} > 0$} 
			\State $\text{Exchange } av_1,\dots, av_k \text{ with } bv_1,\dots, bv_k$  
		\EndIf	
      \doWhile{$g_{max} > 0$}
  \end{algorithmic}
  \end{boxedminipage}
  \vskip1.5pt
\label{alg:kernighan_lin}
\end{algorithm}
Since our implementation solves for multiple graph segments, we define several initial sets and then apply an alpha-beta expansion heuristics by running algorithm $\ref{alg:kernighan_lin}$ on every distinct pair of vertex set. 
Our final $KL$ algorithm we use to solve the multi-cut problem is listed in algorithm $\ref{alg:kl_multiple_segments}$.

\begin{algorithm}[H]
\caption{Kernighan-Lin Multicut Heuristic}
\begin{table}[H]
  \begin{tabular}{@{}lll@{}}
    \textbf{Input:} & Graph \emph{G = (V, E)} \\
        & Number of segments C \\
    & Number of repetitions N  \\
    & Number of dummy vertices D \\
	\textbf{Output:} & Multiple Graph Partition $\left( A_1,\dots, A_N \right)$ 
  \end{tabular} 
\end{table}
\textbf{Procedures:} $KernighanLin(G)$  \\
\setlength{\fboxrule}{0pt} 
\begin{boxedminipage}{1.0\textwidth}
  \begin{algorithmic}[1]
  	\For{$n = 1 : N$}
  	  \State $\text{Split V into C initial balanced sets} A_1,\dots,A_C$
  	  \State $\text{Append D dummy vertices to each initial set} A_k$ 
      \ForAll{$\text{distinct vertex set pair} \left( A_k, A_l\right)$}
        \State $ \text{Form the subgraph } G_{k,l} = (V, E)$
		\State $\text{Run } KernighanLin(G_{k,l}) \text{ and Update G}$
      \EndFor
    \EndFor{$\text{d}$}
  \end{algorithmic}
  \end{boxedminipage}
  \vskip1.5pt
\label{alg:kl_multiple_segments}
\end{algorithm}  
  
% Explain the algorithm above, the alpha-beta swap, error terms and so forth...

\section{Dense Motion Segmentation}
%TODO: use the dense segmentation as an input solve the problem of whole filling using a primal-dual convex algorithm. 


In this section I describe how I have actually Implemented the so far described dual-primal solver for demosaicing a raw image.

One important note in advance. In the discrete case, the following holds true 

\begin{align}
	\nabla^* (y^{n+1}) 
	&= \nabla^{T} (y^{n+1}) \nonumber \\
	&= div(y^{n+1})
\end{align}

So we have to omit a minus one factor. This affects the update rule for $x^{n+1}$ derived previously.

In the previous section we have defined explicit update rules. Aggregating all finding, mainly those from equation $\ref{eq:update_x_n_p_1}$ and $\ref{eq:update_rule_y_n_p_1}$, and plugging them into equation $\ref{eq:update_rules_plain}$ we get our update rules


\begin{align}
	y^{n+1} &= \frac{y^n + \sigma \nabla \bar{x}^{n}}{\max{\left(1,\twonorm{y^n + \sigma \nabla \bar{x}^{n}} \right)}} \nonumber \\
	x^{n+1} &= \frac{x^n - \tau div(y^{n+1}) +  \tau \lambda \Omega g}{1+\tau \lambda \Omega} \\
	\bar{x}^{n+1} &= x^{n+1} + \theta(x^{n+1} - x^n)
\label{eq:final_update_rules_plain}	
\end{align}

I initialized $x_n$ with the mosaiced image $g$, $y^{n}$ with a zeros \footnote{a tensor of dimension $M \times N \times 2$ filled with zeros, where $(M \times N)$ denotes the dimension of one color channel of $g$.} and $\bar{x}^{n}$ also with the mosaiced image $g$.

For computing $\nabla$ I used a forward difference approximation scheme. For computing the divergence operator of the vector-field $y^{n+1}$ I used backward difference approximation scheme. The reason for using a backward difference using a backward difference is to shift back gradients (remember, the divergence is applied to $y^{n+1}$ which is the result of a forward difference. Otherwise, when not altering between a forward-and backward difference we would end up with shifted gradients.

For computing the divergence, I relied on its mathematical definition. For a given vector-field $v = (v_x, v_y)$ the divergence is defined as the following:

\begin{align}
	div(v) = \partial_x v_x + \partial_y v_y
\end{align}

Since in our case we have $v = y^{n+1}$ and $y^{n+1}$ a vector valued function of the form $y^{n+1} = (y_{x}^{n+1}, (y_{y}^{n+1})$ it follows:

\begin{align}
	div(y^{n+1}) 
	&= div((y_{x}^{n+1}) + div((y_{y}^{n+1}) \\
	&= \left( \partial_x y_{x}^{n+1} + \partial_y y_{x}^{n+1} \right) + \left( \partial_x y_{y}^{n+1} + \partial_y y_{y}^{n+1} \right)
\end{align}

Initially, I used the following parameter setting:

\begin{align}
	\lambda &= 1000 \\
	\theta &= 0.5 \\
	\tau &= 2*10^{-3} \\
	\sigma &= \frac{1}{\tau * \sqrt{\norm{K}}}
\label{eq:parameter_set_up}	
\end{align}

With $\sqrt{\norm{K}} = \sqrt{4}$, a strong upper bound for the function $K$\footnote{A proof for the existence of this upper bound can be found in \cite{chambolle2004algorithm}}.

For consistency, a named all functions in my Matlab code the same as in this report. Furthermore I used a fixed number of iterations for computing my iterative demosaiced images.

the final algorithms I have to perform is the following:

For each color-channel $C \in \{R,G,B\}$ Do: Loop until $\norm{\bar{x}^{n+1} - \bar{x}^{n}}$ is small enough do: use parameter setup as defined in equation $\ref{eq:parameter_set_up}$ and then solve the update rules from equation $\ref{eq:final_update_rules_plain}$. Finally, merge all color iterative color channel solutions to a color image.

When computing the gradient and divergence finite approximation schemes, I used a zero padding boundary condition. Since I also tried out this kind of boundary condition in the first report, comparing the results produced in this project which those from the first project is valid.

One last comment: From the definition of the update rules, we see that the value of $\lambda$ directly affects the parameters $\tau$ and $\sigma$. Thus, when changing the value of $\lambda$ we also would have to find new best $\tau$ and $\sigma$ parameters. Hence, changing $\lambda$ also affects the convergence behaviour of the primal dual method. 

\section{Segment Merger}
In this section we describe our technique to reduce oversegmentations produced by our pipeline. This stage is invoked before running the evalaution program, implemented as a post-processing stage.

\begin{figure}[H]
\begin{center}
\subfigure[Ground Truth Segmentation]{
   \includegraphics[width=0.47\linewidth] {implementation/merger/mask}
   \label{fig:merger_result_a}
}
\subfigure[Generated Segmentation]{
   \includegraphics[width=0.47\linewidth] {implementation/merger/oversegmentation}
   \label{fig:merger_result_b}
}
~
\subfigure[Overlay Ground Truth / Segmentation]{
   \includegraphics[width=0.47\linewidth] {implementation/merger/mask_segments_overlay}
   \label{fig:merger_result_c}
}
\subfigure[Merged Segmentation]{
   \includegraphics[width=0.47\linewidth] {implementation/merger/merged}
   \label{fig:merger_result_d}
}

\end{center}
\caption[Segmentation Merger]{Visualization of our segment merger's input and output. As an input it expects a ground truth segmentation (see figure $\ref{fig:merger_result_a}$) and a generated oversegmentation (see figure $\ref{fig:merger_result_b}$). As output it yields the merged segmentation as shown in subfigure $\ref{fig:merger_result_c}$.}
\label{fig:merger_result}
\end{figure}
Usually, the results generated by our pipeline exhibit an oversegmentation when comparing them against their ground truth. An example of such an oversegmentation is illustrated in figure $\ref{fig:merger_result_b}$. Ideally, before evaluating the quality, we would like to refine our over-segmented results in a way to reduce the total number of unnecessary segments. Therefore, we initially attempt to merge segments causing an oversegmentation of an ground truth mask. The goal of the merging phase is to lower the number of total segments but the same time retain meaningful segmentation labels. 

Conceptually, our merging approach is visualized in figure $\ref{fig:merger_result}$. Different approaches how to implement such a merging mechanism are described in literature (LIST SOME REFS HERE). However, we use a much simpler approach. 

A segmentation $S$ is basically a set of points in which every point is assigned to a certain segment identifier. In the following let us denote $S_j$ as the subset of points in $S$ that belong to the segmentation label $j$. Similarly, let $M$ denote the set of all ground truth points with their corresponding mask labels and $M_i$ the set of all points that belong to the ground truth mask $i$. To compute the merged segments we do the following: For every segment $S_j$ in $S$ we determine its best matching ground truth segment $M_i$ as defined in equation $\ref{eq:merging_label_formula}$. 
\begin{equation}
i = \argmaxl_{M_i \in M} \left\vert{S_j \cap M_i}\right\vert
\label{eq:merging_label_formula}
\end{equation}
The idea is to find the ground truth mask $i$ that yields the most point intersections with the currently considered segment $S_j$. Next, we update every point in $S_j$ by setting their segment label to $i$. After doing so we have computed the merged segmentation version of the initial oversegmentation. An example is shown in figure $\ref{fig:merger_result_c}$.



