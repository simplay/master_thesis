# Pipeline

**Given**: A sequence of coherent images forming a video.

**Goal**: Find a segmentation of all moving objects for every frame.

## Ideas 

+ Make use of the conecept of **optical flow**. 
 + Objects that move similarly (similar speed and direction) should belong to the same segmentation class. 
+ Motion is not independent for each frame but as history of poins to make grouping decission.
+ Exploit depth information to enhance the quality of the final segmentation. E.g. compute the optical flow using methods that make use of depth information. Another idea is to adapt the compuation of trajectory similarities in a way that the also use depth cues.

## Computational steps and details:

1. Computation of the optical flow for every given image.
 + The optical flow is a 2d vectorfield in pixel units.
 + Compute the forward- and backward optical flow between every successiv image pair. 
 + The forward flow is defined as the optical flow from image frame t to the image frame t+1 where as the backward flow is the optical flow from image frame t+1 to image frame t.
 + For computing the optical flow between two successive images (both, for forward-and backward flow) we rely on existing flow computation methods.
 + Currently, I use T. Brox' LDOF method and code in order to compute the flow fields.

2. Track points over the whole image sequence.
 + Def(**Point tracking**): For a given feature in **frame t** at location (tx, ty) we compute its corresponding **tracked to** coordinates in **frame (t+1)**.
 + We exploit the previousely computed optical flows for computing such trackings.
 + Note that not every point in a given frame is tracked. 
 + Therefore, we initially compute for each frame the so called **tracking candidates**. Only these points are candidate points in the image worth being tracked.
 + Tracking candidates are computed by applying **Haris corner detector**/filter to the frames.
 + In addition, a grid (with a certain cell size) is applied to the candidates, acting as a selection mask. This reduces the number of tracking candidates (spacial subsampling) drastically and makes the sampling sparser.
 + Each point **p** in a certain frame can be **tracked to** the next frame by applying its corresponding forward flow **ff**: For a given **tracking from** position **p1** (in frame t) we can compute its **tracked to** position **p2**  (in frame t+1) by adding the corresponding forward flow **ff1** (flow from frame t to t+1) to p1. In short `p2 = p1 + ff1`.
 + There are 2 types of points we can use for starting a tracking (to the next frame): Either from points resulting from a **continued tracking** or the so called **tracking candidates** computed in the current frame.
 + The **continued tracking** points are the **tracked to** points resulting from tracking step applied in the previous frame. 
 + Initially, in frame 1, we only can use tracking candidates for computing trackings.
 + Point trackings can start and end in every frame due to occlusion (ostacle blocs some image region) or diocclusion (point tracking is outside of frame).
 + Ending a point tracking means that the latest tracked to position cannot be used any further for later trackings.
 + To make the computation more sparse, we only use those tracking points that are not "too close" to an existing continued tracking. To close is measured in pixel units. All too close candidate points are filtered by applying a selection mask.
 + Occlusion is detected by applyinge backward flow to a continued tracked point and checking whether that resulting point has the same origin as the actual tracked from position.
 + Sine we have images in pixel coordinates (in natural number valued) but the optical flow has float values, we have to make use of bilinear interpolation to correctly compute tracked to positions and so forth.
 
3. Compute Trajectories using the previousely tracked points.
 + Trajectory encode spacial and temporal coherence between point trackings.
 + During the point tracking step, we mark every continued point that belongs to the same tracking with a label (some integer id that is stored for every point).
 + The trajectories are generated by aggregating all points with the same label value and keeping them in the same order as their frame were orderd.
 + The resulting trajectories are stored as a struct that ha a list of points (the continued tracks over their frames), the starting frame index, and their label value.
 
4. Compute similarites between all determined trajectoires.
 + Initially, we discard all too short trajectories (trajectories with fewer than 4 tracked points). Then we form the set of all trajectory pairs and iterate over them.
 + For every pair, we compute their affinity value. The actual computation is described below.
 + The resulting affinity values are stored in a affinity matrix W where the element at position (i,j) represents the affinity between the trajectory i and the trajectory j (remember that trajectories are identified by their label).
 + For computing the affinity between two trajectories we rely on the approach described by T. Brox in his paper _Segmentation of Moving Objects by long term video Analysis_.
 + For every trajectoriy pair (A,B), we find their overlapping tracking points. That are all tracked points from trajectory A that live in the same frame as a tracking point of trajectory B (i.e. the intersection of the tracking points of both trajectories with respect of their frame indices).
 + For these overlapping points we compute their tangents using forward differences for both points. 
 + As proposed by Brox' we do not use the step size of 1 (for the forward diff. scheme), but rather we use a stepsize of T = min(5, number_overlapping_points).
 + In short: `d_t^A = 1/T (x_{t+T}^{A}-x_{t}^{A}, y_{t+T}^{A}-y_{t}^{A})`, analogously for `d_t^B`.
 + We next compute ||d_t^A - d_t^B||^2 / sigma_t^2 for every overlapping point pair for every trajectory pair. We call this resulting value the **temporal distance**.
 + Note that sigma_t denotes the variance of the optical flow at frame t and ||d_t^A - d_t^B||^2 is some measure encoding the change of flow difference between two trajectories.
 + In addition we also compute the spacial average distance **d_sp** between the overlapping points.
 + Last we multiply this spacial distance by the temporal distance for every overlapping point. We take the maximum value between all resulting values from the overlapping points and use it as the **distance** value **between two trajectories**. This resulting value **d^2(A,B)** is the distance between two trajectories.
 + The affinity w(A,B) between two trajectories A, B is equal to `w(A,B) = exp(-lambda*d^2(A,B))`.
 + In addition we also make use of depth information here. For every tracked point we use its depth for computing the tangent values and the spacial distance. 
 + We further use the extrinsic and intrinsic camera calibration information to perform computations in the metric system.
 + Note that this compuation can only be performed if such depth information is available for every frame and the calibartion data is present.
 
5. Compute the segmentation using a special variant of spectral clustering using the trajectory similarities as a cue.
 + We use the resulting affinity matrix W for the following compuations.
 + First we compute the symmetrix laplacian matrix L from W: `L = D^(-1/2) (D-W) D^(-1/2)` where D is the diaganal matrix containing the sam of all row affinity values.
 + We compute the eigenvalues and eigenvectors of the matrix L. We only use the eigenvectors corresponding tho eigenvalues that are below a certain threshold to ensure to use only descriptive eigenvalues.
 + Apply the k-means algorithm to the resulting eigenvectors (currently for a fixed k value). This gives us a clustering.
 + Last, map the clustered values (from the k-means we received the cluster value for every trajectory label) that belong to the labels back to pixel locations using the labels as foreign key.

## Next steps

+ Try out different optical flow methods for generating forward-and backward optical flow fields.
+ Implement a method to generate dense segmentations
+ Compare results to benchmark datasets (see Brox' datasets).
+ Currently, we do not know how we actually should compute the variance value of the optical flow as described in Brox's Paper. TODO: Find an appropriate way.
